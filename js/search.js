// When the user clicks on the search box, we want to toggle the search dropdown
function displayToggleSearch(e) {
  e.preventDefault();
  e.stopPropagation();

  closeDropdownSearch(e);
  
  if (idx === null) {
    console.log("Building search index...");
    prepareIdxAndDocMap();
    console.log("Search index built.");
  }
  const dropdown = document.querySelector("#search-dropdown-content");
  if (dropdown) {
    if (!dropdown.classList.contains("show")) {
      dropdown.classList.add("show");
    }
    document.addEventListener("click", closeDropdownSearch);
    document.addEventListener("keydown", searchOnKeyDown);
    document.addEventListener("keyup", searchOnKeyUp);
  }
}

//We want to prepare the index only after clicking the search bar
var idx = null
const docMap = new Map()

function prepareIdxAndDocMap() {
  const docs = [  
    {
      "title": "Async",
      "url": "/cats-effect/typeclasses/async.html",
      "content": "A Monad that can describe asynchronous or synchronous computations that produce exactly one result. import cats.effect.{LiftIO, Sync} trait Async[F[_]] extends Sync[F] with LiftIO[F] { def async[A](k: (Either[Throwable, A] =&gt; Unit) =&gt; Unit): F[A] } This type class allows the modeling of data types that: Can start asynchronous processes. Can emit one result on completion. Can end in error Important: on the “one result” signaling, this is not an “exactly once” requirement. At this point streaming types can implement Async and such an “exactly once” requirement is only clear in Effect. Therefore the signature exposed by the Async.async builder is this: (Either[Throwable, A] =&gt; Unit) =&gt; Unit Important: such asynchronous processes are not cancelable. See the Concurrent alternative for that. On Asynchrony An asynchronous task represents logic that executes independent of the main program flow, or current callstack. It can be a task whose result gets computed on another thread, or on some other machine on the network. In terms of types, normally asynchronous processes are represented as: (A =&gt; Unit) =&gt; Unit This signature can be recognized in the “Observer pattern” described in the “Gang of Four”, although it should be noted that without an onComplete event (like in the Rx Observable pattern) you can’t detect completion in case this callback can be called zero or multiple times. Some abstractions allow for signaling an error condition (e.g. MonadError data types), so this would be a signature that’s closer to Scala’s Future.onComplete: (Either[Throwable, A] =&gt; Unit) =&gt; Unit And many times the abstractions built to deal with asynchronous tasks also provide a way to cancel such processes, to be used in race conditions in order to cleanup resources early: (A =&gt; Unit) =&gt; Cancelable This is approximately the signature of JavaScript’s setTimeout, which will return a “task ID” that can be used to cancel it. Important: this type class in particular is NOT describing cancelable async processes, see the Concurrent type class for that. async The async method has an interesting signature that is nothing more than the representation of a callback-based API function. For example, consider the following example having an API that returns a Future[String] as a response: import cats.effect.{IO, Async} import scala.concurrent.ExecutionContext.Implicits.global import scala.concurrent.Future val apiCall = Future.successful(\"I come from the Future!\") val ioa: IO[String] = Async[IO].async { cb =&gt; import scala.util.{Failure, Success} apiCall.onComplete { case Success(value) =&gt; cb(Right(value)) case Failure(error) =&gt; cb(Left(error)) } } ioa.unsafeRunSync() In the example above ioa will have a successful value A or it will be raise an error in the IO context."
    } ,    
    {
      "title": "Concurrency Basics",
      "url": "/cats-effect/concurrency/basics.html",
      "content": "Introduction Concurrency is not an easy topic. There are a lot of concepts involved and the vocabulary might be hard to search. This post intends to gather and explain some of the most important ideas and serve as a reference point for understanding the basics of concurrency. It is focused on using Scala with libraries in Cats-Effect ecosystem. Dictionary Parallelism Using multiple computational resources (like more processor cores) to perform a computation faster, usually executing at the same time. Example: summing a list of Integers by dividing it in half and calculating both halves in parallel. Main concern: efficiency. Concurrency Multiple tasks interleaved. Concurrency doesn’t have to be multithreaded. We can write concurrent applications on single processor using methods such as event loops. Example: Communicating with external services through HTTP. Main concern: interaction with multiple, independent and external agents. CPU-bound task Operation that mostly requires processor resources to finish its computation. IO-bound task Operation that mostly does I/O and it doesn’t depend on your computation resources, e.g. waiting for disk operation to finish or external service to answer your request. Non-terminating task Task that will never signal its result. A task can be non-terminating without blocking threads or consuming CPU. IO.never *&gt; IO(println(\"done\")) The above will never print “done”, block a thread (unless .unsafeRunSync is run on it), or consume CPU after its creation. Threads Threading (on JVM) Threads in JVM map 1:1 to the operating system’s native threads. Calling new Thread() also creates an operating system thread. We can create many of them (as long as we can fit them in the memory) but we can only execute 1 per core at the given time. Others have to wait for their turn. If we try to run too many threads at once we will suffer because of many context switches. Before any thread can start doing real work, the OS needs to store state of earlier task and restore the state for the current one. This cleanup has nontrivial cost. The most efficient situation for CPU-bound tasks is when we execute as many threads as the number of available cores because we can avoid this overhead. For the above reasons, synchronous execution can have better throughput than parallel execution. If you parallelize it too much, it won’t make your code magically faster. The overhead of creating or switching threads is often greater than the speedup, so make sure to benchmark. Remember that threads are scarce resource on JVM. If you exploit them at every opportunity it may turn out that your most performance critical parts of the application suffer because the other part is doing a lot of work in parallel, taking precious native threads. Thread Pools Creating a Thread has a price to it. The overhead depends on the specific JVM and OS, but it involves making too many threads for short-lived tasks is very inefficient . It may turn out that process of creating thread and possible context switches has higher costs than the task itself. Furthermore, having too many threads means that we can eventually run out of memory and that they are competing for CPU, slowing down the entire application. It is advised to use thread pools created from java.util.concurrent.Executor. A thread pool consists of work queue and a pool of running threads. Every task (Runnable) to execute is placed in the work queue and the threads that are governed by the pool take it from there to do their work. In Scala, we avoid explicitly working with Runnable and use abstractions that do that under the hood (Future and IO implementations). Thread pools can reuse and cache threads to prevent some of the problems mentioned earlier. Choosing Thread Pool We can configure thread pools in multiple ways: Bounded Limiting number of available threads to certain amount. Example could be newSingleThreadExecutor to execute only one task at the time or limiting number of threads to number of processor cores for CPU-bound tasks. Unbounded No maximum limit of available threads. Note that this is dangerous because we could run out of memory by creating too many threads, so it’s important to use cached pool (allowing to reuse existing threads) with keepalive time (to remove useless threads) and control number of tasks to execute by other means (backpressure, rate limiters). Despite those dangers it is still very useful for blocking tasks. In limited thread pool if we block too many threads which are waiting for callback from other (blocked) thread for a long time we risk getting deadlock that prevents any new tasks from starting their work. For more, read Daniel Spiewak’s gist. Blocking Threads As a rule we should never block threads, but sometimes we have to work with interface that does it. Blocking a thread means that it is being wasted and nothing else can be scheduled to run on it. As mentioned, this can be very dangerous and it’s best to use dedicated thread pool for blocking operations. This way they won’t interfere with CPU-bound part of application. Blocker[IO] can be used to safely handle blocking operations in an explicit way. import cats.effect.{Blocker, ContextShift, IO} import scala.concurrent.ExecutionContext implicit val contextShift: ContextShift[IO] = IO.contextShift(ExecutionContext.global) def blockingOp: IO[Unit] = IO(/* blocking op*/ ()) def doSth(): IO[Unit] = IO(/* do something */ ()) val prog = Blocker[IO].use { blocker =&gt; for { _ &lt;- blocker.blockOn(blockingOp) // executes on blocker, backed by cached thread pool _ &lt;- doSth() // executes on contextShift } yield () } In most circumstances use a shared Blocker when carrying out blocking operations. Other resource with good practices regarding working with blocked threads is this section of Monix documentation. Green Threads There are more types of threads and they depend on the platform. One of them is green thread. The main difference between model represented by JVM Threads and Green Threads is that the latter aren’t scheduled on OS level. They are much more lightweight, which allows starting a lot of them without many issues. They are often characterized by cooperative multitasking which means the thread decides when it’s giving up control instead of being forcefully preempted, as happens on the JVM. This term is important for Cats Effect, whose Fiber and shift design have a lot of similarities with this model. Thread Scheduling Working with cats.effect.IO you should notice a lot of calls to IO.shift, described in Thread Shifting section in IO documentation This function allows to shift computation to different thread pool or simply send it to current ExecutionContext to schedule it again. This is often called introducing asynchronous boundary. While the first use case is probably easy to imagine, the second one might be more confusing. It is helpful to actually understand what is happening behind the scenes during shift. The Essential term is thread scheduling. Since we can’t run all our threads in parallel all the time, they each get their own slice of time to execute, interleaving with the rest of them so every thread has a chance to run. When it is time to change threads, the currently running thread is preempted. It saves its state and the context switch happens. This is a bit different when using thread pools (ExecutionContexts), because they are in charge of scheduling threads from their own pool. If there is one thread running, it won’t change until it terminates or higher priority thread is ready to start doing work. Note that IO without any shifts is considered one task, so if it’s infinite IO, it could hog the thread forever and if we use single threaded pool, nothing else will ever run on it! In other words, IO is executing synchronously until we call IO.shift or use function like parSequence. In terms of individual thread pools, we can actually treat IO like green thread with cooperative multitasking. Instead of preemption, we can decide when we yield to other pending fibers from the same pool by calling shift. Calling IO.shift schedules the work again, so if there are other IOs waiting to execute, they can have their chance. From the thread pool’s point of view, the process of yielding to other fibers can be described like this: When shift is called on some fiber: Remove that fiber from its current thread and put it in the pool of pending fibers For each available thread (including the one from the previous step), assign it one of the pending fibers from the pool Allowing different fibers to advance their work is called fairness. Let’s illustrate this: import java.util.concurrent.Executors import cats.effect.{ContextShift, Fiber, IO} import scala.concurrent.ExecutionContext val ecOne = ExecutionContext.fromExecutor(Executors.newSingleThreadExecutor()) val ecTwo = ExecutionContext.fromExecutor(Executors.newSingleThreadExecutor()) val csOne: ContextShift[IO] = IO.contextShift(ecOne) val csTwo: ContextShift[IO] = IO.contextShift(ecTwo) def infiniteIO(id: Int)(cs: ContextShift[IO]): IO[Fiber[IO, Unit]] = { def repeat: IO[Unit] = IO(println(id)).flatMap(_ =&gt; repeat) repeat.start(cs) } We have two single threaded ExecutionContexts (wrapped in ContextShift) and a function that will run IO, forever printing its identifier. Note repeat.start and return type of IO[Fiber[IO, Unit]] which means that we run this computation in the background. It will run on thread pool provided by cs, which we will pass explicitly: val prog = for { _ &lt;- infiniteIO(1)(csOne) _ &lt;- infiniteIO(11)(csOne) } yield () prog.unsafeRunSync() It will never print 11 despite using .start! Why? The ecOne execution context executes its IO on the only thread it has, but needs to wait for its completion before it can schedule the other one. How about two thread pools? val program = for { _ &lt;- infiniteIO(1)(csOne) _ &lt;- infiniteIO(11)(csOne) _ &lt;- infiniteIO(2)(csTwo) _ &lt;- infiniteIO(22)(csTwo) } yield () program.unsafeRunSync() Now it will keep printing both 1 and 2 but neither 11 nor 22. What changed? Those thread pools are independent and interleave because of thread scheduling done by the operating system. Basically, the thread pool decides which task gets a thread to run but the OS decides what is actually evaluating on the CPU. Let’s introduce asynchronous boundaries: import java.util.concurrent.Executors import cats.effect.{ContextShift, Fiber, IO} import scala.concurrent.ExecutionContext val ecOne = ExecutionContext.fromExecutor(Executors.newSingleThreadExecutor()) val ecTwo = ExecutionContext.fromExecutor(Executors.newSingleThreadExecutor()) val csOne: ContextShift[IO] = IO.contextShift(ecOne) val csTwo: ContextShift[IO] = IO.contextShift(ecTwo) def infiniteIO(id: Int)(implicit cs: ContextShift[IO]): IO[Fiber[IO, Unit]] = { def repeat: IO[Unit] = IO(println(id)).flatMap(_ =&gt; IO.shift *&gt; repeat) repeat.start } val prog = for { _ &lt;- infiniteIO(1)(csOne) _ &lt;- infiniteIO(11)(csOne) _ &lt;- infiniteIO(2)(csTwo) _ &lt;- infiniteIO(22)(csTwo) } yield () prog.unsafeRunSync() Notice the IO.shift *&gt; repeat call. *&gt; means that we execute first operation, ignore its result and then call repeat. Now everything is fair, as we can see each of those numbers printed on the screen. Calling IO.shift fixed the problem because when the currently running IO was rescheduled, it gave an opportunity to execute the other one. It probably sounds quite complex and cumbersome to keep track of it yourself but once you understand fundamentals this explicity can be a great virtue of cats.effect.IO. Knowing what exactly happens in concurrent scenarios in your application just by reading the piece of code can really speedup debugging process or even allow to get it right the first time. Fortunately cats.effect.IO doesn’t always require to do it manually. Operations like race, parMapN or parTraverse introduce asynchronous boundary at the beginning, but if you have limited thread pool and long running tasks, keep fairness in mind. Scala’s Future is optimized for fairness, doing shift equivalent after each map or flatMap. We wouldn’t have the problem described above but doing it too much results in putting a lot of pressure on scheduler causing low throughput. In typical purely functional programs we have many flatMaps because our entire application is just one big IO composed of many smaller ones. Constant shifting is not feasible but there’s always the option to do it if our application has strict latency requirements. If you are looking for less manual work - monix.eval.Task is great middleground which by default shifts tasks automatically in batches preserving both great throughput and decent latency off the shelf and exposes very rich configuration options if you have more advanced use case. Asynchronous / Semantic blocking Sometimes we use term semantic blocking or asynchronous blocking which is different than blocking thread. It means that we suspend our IO/Task waiting for some action to happen (e.g. Deferred.get waits until the result is available) without blocking a threads. Other IOs are free to execute on the thread in the meantime. This is further explained in Fabio Labella’s comment. It is important to recognize that not all I/O operations are blocking and need to execute on dedicated thread pool. For instance we can have HTTP requests using non-blocking client such as http4s with Blaze, which uses non-blocking network I/O and is free to execute on a “normal” pool."
    } ,    
    {
      "title": "Bracket",
      "url": "/cats-effect/typeclasses/bracket.html",
      "content": "Bracket is an extension of MonadError exposing the bracket operation, a generalized abstracted pattern of safe resource acquisition and release in the face of errors or interruption. Important note, throwing in release function is undefined since the behavior is left to the concrete implementations (ex. cats-effect Bracket[IO], Monix Bracket[Task] or ZIO). import cats.MonadError sealed abstract class ExitCase[+E] trait Bracket[F[_], E] extends MonadError[F, E] { def bracketCase[A, B](acquire: F[A])(use: A =&gt; F[B]) (release: (A, ExitCase[E]) =&gt; F[Unit]): F[B] // Simpler version, doesn't distinguish b/t exit conditions def bracket[A, B](acquire: F[A])(use: A =&gt; F[B]) (release: A =&gt; F[Unit]): F[B] }"
    } ,    
    {
      "title": "Clock",
      "url": "/cats-effect/datatypes/clock.html",
      "content": "Clock provides the current time, as a pure alternative to: Java’s System.currentTimeMillis for getting the “real-time clock” and System.nanoTime for a monotonic clock useful for time measurements JavaScript’s Date.now() and performance.now() The reason for providing this data type is two-fold: the exposed functions are pure, with the results suspended in F[_], no reason to reinvent the wheel and write your own wrappers requiring this data type as a restriction means that code using Clock can have alternative implementations injected; for example time passing can be simulated in tests, such that time-based logic can be tested much more deterministically and with better performance, without actual delays happening The interface looks like this: import scala.concurrent.duration.TimeUnit trait Clock[F[_]] { def realTime(unit: TimeUnit): F[Long] def monotonic(unit: TimeUnit): F[Long] } Important: this is NOT a type class, meaning that there is no coherence restriction. This is because the ability to inject custom implementations in time-based logic is essential. Example: import cats.effect._ import cats.syntax.all._ import scala.concurrent.duration.MILLISECONDS def measure[F[_], A](fa: F[A]) (implicit F: Sync[F], clock: Clock[F]): F[(A, Long)] = { for { start &lt;- clock.monotonic(MILLISECONDS) result &lt;- fa finish &lt;- clock.monotonic(MILLISECONDS) } yield (result, finish - start) }"
    } ,    
    {
      "title": "ConcurrentEffect",
      "url": "/cats-effect/typeclasses/concurrent-effect.html",
      "content": "Type class describing effect data types that are cancelable and can be evaluated concurrently. In addition to the algebras of Concurrent and Effect, instances must also implement the ConcurrentEffect.runCancelable operation that triggers the evaluation, suspended in the SyncIO context, but that also returns a token that can be used for canceling the running computation. Note this is the safe and generic version of IO.unsafeRunCancelable. import cats.effect.{Concurrent, Effect, IO, CancelToken, SyncIO} trait ConcurrentEffect[F[_]] extends Concurrent[F] with Effect[F] { def runCancelable[A](fa: F[A])(cb: Either[Throwable, A] =&gt; IO[Unit]): SyncIO[CancelToken[F]] } This runCancelable operation actually mirrors the cancelable builder in Concurrent. With the runCancelable and cancelable pair one is then able to convert between ConcurrentEffect data types: import cats.effect._ def convert[F[_], G[_], A](fa: F[A]) (implicit F: ConcurrentEffect[F], G: Concurrent[G]): G[A] = { G.cancelable { cb =&gt; val token = F.runCancelable(fa)(r =&gt; IO(cb(r))).unsafeRunSync() convert[F, G, Unit](token) } }"
    } ,    
    {
      "title": "Concurrent",
      "url": "/cats-effect/typeclasses/concurrent.html",
      "content": "Type class for Async data types that are cancelable and can be started concurrently. Thus this type class allows abstracting over data types that: Implement the Async algebra, with all its restrictions. Can provide logic for cancellation, to be used in race conditions in order to release resources early (in its Concurrent.cancelable builder). Due to these restrictions, this type class also affords to describe a Concurrent.start operation that can start async processing, suspended in the context of F[_] and that can be canceled or joined. Without cancellation being baked in, we couldn’t afford to do it. import cats.effect.{Async, Fiber, CancelToken} trait Concurrent[F[_]] extends Async[F] { def start[A](fa: F[A]): F[Fiber[F, A]] def race[A, B](lh: F[A], rh: F[B]): F[Either[A, B]] def racePair[A, B](lh: F[A], rh: F[B]): F[Either[(A, Fiber[F, B]), (Fiber[F, A], B)]] def cancelable[A](k: (Either[Throwable, A] =&gt; Unit) =&gt; CancelToken[F]): F[A] } Notes: this type class is defined by start and by racePair race is derived from racePair cancelable is derived from asyncF and from bracketCase, however it is expected to be overridden in instances for optimization purposes Cancelable Builder The signature exposed by the Concurrent.cancelable builder is this: (Either[Throwable, A] =&gt; Unit) =&gt; CancelToken[F] CancelToken[F] is simply an alias for F[Unit] and is used to represent a cancellation action which will send a signal to the producer, that may observe it and cancel the asynchronous process. On Cancellation Simple asynchronous processes, like Scala’s Future, can be described with this very basic and side-effectful type and you should recognize what is more or less the signature of Future.onComplete or of Async.async (minus the error handling): (A =&gt; Unit) =&gt; Unit But many times the abstractions built to deal with asynchronous tasks can also provide a way to cancel such processes, to be used in race conditions in order to cleanup resources early, so a very basic and side-effectful definition of asynchronous processes that can be canceled would be: (A =&gt; Unit) =&gt; Cancelable This is approximately the signature of JavaScript’s setTimeout, which will return a “task ID” that can be used to cancel it. Or of Java’s ScheduledExecutorService.schedule, which will return a Java ScheduledFuture that has a .cancel() operation on it. Similarly, for Concurrent data types, we can provide cancellation logic, that can be triggered in race conditions to cancel the on-going processing, only that Concurrent’s cancelable token is an action suspended in a CancelToken[F], which is nothing more than an F[Unit]. See IO.cancelable. Suppose you want to describe a “sleep” operation, like that described by Timer to mirror Java’s ScheduledExecutorService.schedule or JavaScript’s setTimeout: def sleep(d: FiniteDuration): F[Unit] This signature is in fact incomplete for data types that are not cancelable, because such equivalent operations always return some cancellation token that can be used to trigger a forceful interruption of the timer. This is not a normal “dispose” or “finally” clause in a try/catch block, because “cancel” in the context of an asynchronous process is “concurrent” with the task’s own run-loop. To understand what this means, consider that in the case of our sleep as described above, on cancellation we’d need a way to signal to the underlying ScheduledExecutorService to forcefully remove the scheduled Runnable from its internal queue of scheduled tasks, “before” its execution. Therefore, without a cancelable data type, a safe signature needs to return a cancellation token, so it would look like this: def sleep(d: FiniteDuration): F[(F[Unit], F[Unit])] This function is returning a tuple, with one F[Unit] to wait for the completion of our sleep and a second F[Unit] to cancel the scheduled computation in case we need it. This is in fact the shape of Fiber’s API. And this is exactly what the Concurrent.start operation returns. The difference between a Concurrent data type and one that is only Async is that you can go from any F[A] to a F[Fiber[F, A]], to participate in race conditions and that can be canceled should the need arise, in order to trigger an early release of allocated resources. Thus a Concurrent data type can safely participate in race conditions, whereas a data type that is only Async cannot without exposing and forcing the user to work with cancellation tokens. An Async data type cannot expose for example a start operation that is safe."
    } ,    
    {
      "title": "ContextShift",
      "url": "/cats-effect/datatypes/contextshift.html",
      "content": "ContextShift is the pure equivalent to: Scala’s ExecutionContext Java’s Executor JavaScript’s setTimeout(0) or setImmediate It provides the means to do cooperative yielding, or on top of the JVM to switch thread-pools for execution of blocking operations or other actions that are problematic. The interface looks like this: import scala.concurrent.ExecutionContext trait ContextShift[F[_]] { def shift: F[Unit] def evalOn[A](ec: ExecutionContext)(f: F[A]): F[A] } Important: this is NOT a type class, meaning that there is no coherence restriction. This is because the ability to customize the thread-pool used for shift is essential on top of the JVM at least. shift The shift operation is an effect that triggers a logical fork. For example, say we wanted to ensure that the current thread isn’t occupied forever on long running operations, we could do something like this: import cats.effect._ import cats.syntax.all._ def fib[F[_]](n: Int, a: Long = 0, b: Long = 1) (implicit F: Sync[F], cs: ContextShift[F]): F[Long] = { F.suspend { val next = if (n &gt; 0) fib(n - 1, b, a + b) else F.pure(a) // Triggering a logical fork every 100 iterations if (n % 100 == 0) cs.shift *&gt; next else next } } evalOn The evalOn operation is about executing a side effectful operation on a specific ExecutionContext, but then “return” to the “default” thread-pool or run-loop for the bind continuation. import java.util.concurrent.Executors import scala.concurrent.ExecutionContext import cats.effect._ def blockingThreadPool[F[_]](implicit F: Sync[F]): Resource[F, ExecutionContext] = Resource(F.delay { val executor = Executors.newCachedThreadPool() val ec = ExecutionContext.fromExecutor(executor) (ec, F.delay(executor.shutdown())) }) def readName[F[_]](implicit F: Sync[F]): F[String] = F.delay { println(\"Enter your name: \") scala.io.StdIn.readLine() } object MyApp extends IOApp { def run(args: List[String]) = { val name = blockingThreadPool[IO].use { ec =&gt; // Blocking operation, executed on special thread-pool contextShift.evalOn(ec)(readName[IO]) } for { n &lt;- name _ &lt;- IO(println(s\"Hello, $n!\")) } yield ExitCode.Success } } Blocker Blocker provides an ExecutionContext that is intended for executing blocking tasks and integrates directly with ContextShift. The previous example with Blocker looks like this: import cats.effect._ def readName[F[_]: Sync: ContextShift](blocker: Blocker): F[String] = // Blocking operation, executed on special thread-pool blocker.delay { println(\"Enter your name: \") scala.io.StdIn.readLine() } object MyApp extends IOApp { def run(args: List[String]) = { val name = Blocker[IO].use { blocker =&gt; readName[IO](blocker) } for { n &lt;- name _ &lt;- IO(println(s\"Hello, $n!\")) } yield ExitCode.Success } } In this version, Blocker was passed as an argument to readName to ensure the constructed task is never used on a non-blocking execution context."
    } ,    
    {
      "title": "Deferred",
      "url": "/cats-effect/concurrency/deferred.html",
      "content": "A purely functional synchronization primitive which represents a single value which may not yet be available. When created, a Deferred is empty. It can then be completed exactly once, and never be made empty again. abstract class Deferred[F[_], A] { def get: F[A] def complete(a: A): F[Unit] } Expected behavior of get get on an empty Deferred will block until the Deferred is completed get on a completed Deferred will always immediately return its content get is cancelable if F[_] implements Concurrent and if the Deferred value was built via the normal apply (and not via uncancelable); and on cancellation it will unsubscribe the registered listener, an operation that’s possible for as long as the Deferred value isn’t complete Expected behavior of complete complete(a) on an empty Deferred will set it to a, and notify any and all readers currently blocked on a call to get. complete(a) on a Deferred that has already been completed will not modify its content, and result in a failed F. Albeit simple, Deferred can be used in conjunction with Ref to build complex concurrent behaviour and data structures like queues and semaphores. Finally, the blocking mentioned above is semantic only, no actual threads are blocked by the implementation. Only Once Whenever you are in a scenario when many processes can modify the same value but you only care about the first one in doing so and stop processing, then this is a great use case of Deferred[F, A]. Two processes will try to complete at the same time but only one will succeed, completing the deferred primitive exactly once. The loser one will raise an error when trying to complete a deferred already completed and automatically be canceled by the IO.race mechanism, that’s why we call attempt on the evaluation. import cats.effect.IO import cats.effect.concurrent.Deferred import cats.syntax.all._ import scala.concurrent.ExecutionContext // Needed for `start` or `Concurrent[IO]` and therefore `parSequence` implicit val cs = IO.contextShift(ExecutionContext.global) def start(d: Deferred[IO, Int]): IO[Unit] = { val attemptCompletion: Int =&gt; IO[Unit] = n =&gt; d.complete(n).attempt.void List( IO.race(attemptCompletion(1), attemptCompletion(2)), d.get.flatMap { n =&gt; IO(println(show\"Result: $n\")) } ).parSequence.void } val program: IO[Unit] = for { d &lt;- Deferred[IO, Int] _ &lt;- start(d) } yield () Cancellation Deferred is a cancelable data type, if the underlying F[_] is capable of it. This means that cancelling a get will unsubscribe the registered listener and can thus avoid memory leaks. However Deferred can also work with Async data types, or in situations where the cancelable behavior isn’t desirable. To do so you can use the uncancelable builder: Deferred.uncancelable[IO, Int] The restriction on the uncancelable builder is just Async, whereas the restriction on the normal apply builder is Concurrent."
    } ,    
    {
      "title": "Effect",
      "url": "/cats-effect/typeclasses/effect.html",
      "content": "A Monad that can suspend side effects into the F[_] context and supports lazy and potentially asynchronous evaluation. import cats.effect.{Async, IO, SyncIO} trait Effect[F[_]] extends Async[F] { def runAsync[A](fa: F[A])(cb: Either[Throwable, A] =&gt; IO[Unit]): SyncIO[Unit] } This type class is describing data types that: Implement the Async algebra. Implement a lawful Effect.runAsync operation that triggers the evaluation (in the context of SyncIO). Note: this is the safe and generic version of IO.unsafeRunAsync (aka Haskell’s unsafePerformIO). runAsync It represents the intention to evaluate a given effect in the context of F[_] asynchronously giving you back a SyncIO[A]. Eg.: import cats.effect.{Effect, SyncIO, IO} val task = IO(\"Hello World!\") val ioa: SyncIO[Unit] = Effect[IO].runAsync(task) { case Right(value) =&gt; IO(println(value)) case Left(error) =&gt; IO.raiseError(error) } ioa.unsafeRunSync()"
    } ,    
    {
      "title": "Fiber",
      "url": "/cats-effect/datatypes/fiber.html",
      "content": "It represents the (pure) result of an Async data type (e.g. IO) being started concurrently and that can be either joined or canceled. You can think of fibers as being lightweight threads, a fiber being a concurrency primitive for doing cooperative multi-tasking. trait Fiber[F[_], A] { def cancel: F[Unit] def join: F[A] } For example a Fiber value is the result of evaluating IO.start: import cats.effect.{Fiber, IO} import scala.concurrent.ExecutionContext.Implicits.global // Needed for `start` implicit val ctx = IO.contextShift(global) val io = IO(println(\"Hello!\")) val fiber: IO[Fiber[IO, Unit]] = io.start Usage example: import cats.effect.{ContextShift, IO} import scala.concurrent.ExecutionContext implicit val contextShift: ContextShift[IO] = IO.contextShift(ExecutionContext.global) val launchMissiles: IO[Unit] = IO.raiseError(new Exception(\"boom!\")) val runToBunker = IO(println(\"To the bunker!!!\")) for { fiber &lt;- launchMissiles.start _ &lt;- runToBunker.handleErrorWith { error =&gt; // Retreat failed, cancel launch (maybe we should // have retreated to our bunker before the launch?) fiber.cancel *&gt; IO.raiseError(error) } aftermath &lt;- fiber.join } yield aftermath"
    } ,    
    {
      "title": "Concurrency",
      "url": "/cats-effect/concurrency/",
      "content": "Concurrency Concurrency Basics: overview of important concepts related to Concurrency. Deferred: pure concurrency primitive built on top of scala.concurrent.Promise. MVar: a purely functional concurrency primitive that works like a concurrent queue Ref: pure concurrency primitive built on top of java.util.concurrent.atomic.AtomicReference. Semaphore: a pure functional semaphore."
    } ,    
    {
      "title": "Tracing",
      "url": "/cats-effect/tracing/",
      "content": "Introduction Tracing is an advanced feature of IO that offers insight into the execution graph of a fiber. This unlocks a lot of power for developers in the realm of debugging and introspection, not only in local development environments but also in critical production settings. A notable pain point of working with asynchronous code on the JVM is that stack traces no longer provide valuable context of the execution path that a program takes. This limitation is even more pronounced with Scala’s Future (pre- 2.13), where an asynchronous boundary is inserted after each operation. IO suffers a similar problem, but even a synchronous IO program’s stack trace is polluted with the details of the IO run-loop. IO solves this problem by collecting a stack trace at various IO operations that a fiber executes, and knitting them together to produce a more coherent view of the fiber’s execution path. For example, here is a trace of a sample program that is running in cached stack tracing mode: IOTrace: 19 frames captured ├ flatMap @ org.simpleapp.examples.Main$.program (Main.scala:53) ├ map @ org.simpleapp.examples.Main$.foo (Main.scala:46) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:45) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:44) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:43) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:42) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:41) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:40) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:39) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:38) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:37) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:36) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:35) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:34) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:33) ├ flatMap @ org.simpleapp.examples.Main$.foo (Main.scala:32) ├ flatMap @ org.simpleapp.examples.Main$.program (Main.scala:53) ╰ ... (3 frames omitted) However, fiber tracing isn’t limited to collecting stack traces. Tracing has many use cases that improve developer experience and aid in understanding how our applications work. These features are described below. A bolded name indicates that the feature has been merged into master. Asynchronous stack tracing. This is essentially what is described above, where stack frames are collected across asynchronous boundaries for a given fiber. Combinator inference. Stack traces can be walked to determine what combinator was actually called by user code. For example, void and as are combinators that are derived from map, and should appear in the fiber trace rather than map. Enhanced exceptions. Exceptions captured by the IO runtime can be augmented with async stack traces to produce more relevant stack traces. Intermediate values. The intermediate values that an IO program encounters can be converted to a string to render. This can aid in understanding the actions that a program takes. Thread tracking. A fiber is scheduled on potentially many threads throughout its lifetime. Knowing what thread a fiber is running on, and when it shifts threads is a powerful tool for understanding and debugging the concurrency of an application. Tree rendering. By collecting a trace of all IO operations, a pretty tree or graph can be rendered to visualize fiber execution. Fiber identity. Fibers, like threads, are unique and can therefore assume an identity. If user code can observe fiber identity, powerful observability tools can be built on top of it. For example, another shortcoming of asynchronous code is that it becomes tedious to correlate log messages across asynchronous boundaries (thread IDs aren’t very useful). With fiber identity, log messages produced by a single fiber can be associated with a unique, stable identifier. Fiber ancestry graph. If fibers can assume an identity, an ancestry graph can be formed, where nodes are fibers and edges represent a fork/join relationship. Asynchronous deadlock detection. Even when working with asynchronously blocking code, fiber deadlocks aren’t impossible. Being able to detect deadlocks or infer when a deadlock can happen makes writing concurrent code much easier. Live fiber trace dumps. Similar to JVM thread dumps, the execution status and trace information of all fibers in an application can be extracted for debugging purposes. Monad transformer analysis. As note of caution, fiber tracing generally introduces overhead to applications in the form of higher CPU usage, memory and GC pressure. Always remember to performance test your applications with tracing enabled before deploying it to a production environment! Asynchronous stack tracing Configuration The stack tracing mode of an application is configured by the system property cats.effect.stackTracingMode. There are three stack tracing modes: DISABLED, CACHED and FULL. These values are case-insensitive. To prevent unbounded memory usage, stack traces for a fiber are accumulated in an internal buffer as execution proceeds. If more traces are collected than the buffer can retain, then the older traces will be overwritten. The default size for the buffer is 16, but can be changed via the system property cats.effect.traceBufferLogSize. Note that this property is expressed as a logarithm of a power of two! For example, to enable full stack tracing and a trace buffer size of 32, specify the following system properties: -Dcats.effect.stackTracingMode=full -Dcats.effect.traceBufferLogSize=5 DISABLED No tracing is instrumented by the program and so incurs negligible impact to performance. If a trace is requested, it will be empty. CACHED When cached stack tracing is enabled, a stack trace is captured and cached for every map, flatMap and async call in a program. The stack trace cache is indexed by the lambda class reference, so cached tracing may produce inaccurate fiber traces under several scenarios: Monad transformer composition A named function is supplied to map, async or flatMap at multiple call-sites We measured less than a 30% performance hit when cached tracing is enabled for a completely synchronous IO program, but it will most likely be much less for any program that performs any sort of I/O. We strongly recommend benchmarking applications that make use of tracing. This is the recommended mode to run in most production applications and is enabled by default. FULL When full stack tracing is enabled, a stack trace is captured for most IO combinators including pure, delay, suspend, raiseError as well as those traced in cached mode. Stack traces are collected on every invocation, so naturally most programs will experience a significant performance hit. This mode is mainly useful for debugging in development environments. Requesting and printing traces Once the global tracing flag is configured, IO programs will automatically begin collecting traces. The trace for a fiber can be accessed at any point during its execution via the IO.trace combinator. This is the IO equivalent of capturing a thread’s stack trace. After we have a fiber trace, we can print it to the console, not unlike how Java exception stack traces are printed with printStackTrace. printFiberTrace can be called to print fiber traces to the consoles. Printing behavior can be customized by passing in a PrintingOptions instance. By default, a fiber trace is rendered in a very compact presentation that includes the most relevant stack trace element from each fiber operation. import cats.effect.IO def program: IO[Unit] = for { _ &lt;- IO(println(\"Started the program\")) trace &lt;- IO.trace _ &lt;- trace.printFiberTrace() } yield () Keep in mind that the scope and amount of information that traces hold will change over time as additional fiber tracing features are merged into master. Enhanced exceptions The stack trace of an exception caught by the IO runloop looks similar to the following output: java.lang.Throwable: A runtime exception has occurred at org.simpleapp.examples.Main$.b(Main.scala:28) at org.simpleapp.examples.Main$.a(Main.scala:25) at org.simpleapp.examples.Main$.$anonfun$foo$11(Main.scala:37) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18) at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:103) at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:440) at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:461) at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:399) at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36) at cats.effect.internals.PoolUtils$$anon$2$$anon$3.run(PoolUtils.scala:52) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) It includes stack frames that are part of the IO runloop, which are generally not of interest to users of the library. When asynchronous stack tracing is enabled, the IO runloop is capable of augmenting the stack traces of caught exceptions to include frames from the asynchronous stack traces. For example, the augmented version of the above stack trace looks like the following: java.lang.Throwable: A runtime exception has occurred at org.simpleapp.examples.Main$.b(Main.scala:28) at org.simpleapp.examples.Main$.a(Main.scala:25) at org.simpleapp.examples.Main$.$anonfun$foo$11(Main.scala:37) at map @ org.simpleapp.examples.Main$.$anonfun$foo$10(Main.scala:37) at flatMap @ org.simpleapp.examples.Main$.$anonfun$foo$8(Main.scala:36) at flatMap @ org.simpleapp.examples.Main$.$anonfun$foo$6(Main.scala:35) at flatMap @ org.simpleapp.examples.Main$.$anonfun$foo$4(Main.scala:34) at flatMap @ org.simpleapp.examples.Main$.$anonfun$foo$2(Main.scala:33) at flatMap @ org.simpleapp.examples.Main$.foo(Main.scala:32) at flatMap @ org.simpleapp.examples.Main$.program(Main.scala:42) at as @ org.simpleapp.examples.Main$.run(Main.scala:48) at main$ @ org.simpleapp.examples.Main$.main(Main.scala:22) Note that the relevant stack frames from the call-site of the user code is preserved, but all IO-related stack frames are replaced with async stack trace frames. This feature is controlled by the system property cats.effect.enhancedExceptions. It is enabled by default. -Dcats.effect.enhancedExceptions=false Complete example Here is a sample program that demonstrates tracing in action. // Pass the following system property to your JVM: // -Dcats.effect.stackTracingMode=full import cats.effect.tracing.PrintingOptions import cats.syntax.all._ import cats.effect.{ExitCode, IO, IOApp} import scala.util.Random object Example extends IOApp { val options = PrintingOptions.Default .withShowFullStackTraces(true) .withMaxStackTraceLines(8) def fib(n: Int, a: Long = 0, b: Long = 1): IO[Long] = IO(a + b).flatMap { b2 =&gt; if (n &gt; 0) fib(n - 1, b, b2) else IO.pure(b2) } def program: IO[Unit] = for { x &lt;- fib(20) _ &lt;- IO(println(s\"The 20th fibonacci number is $x\")) _ &lt;- IO(Random.nextBoolean()).ifM(IO.raiseError(new Throwable(\"\")), IO.unit) } yield () override def run(args: List[String]): IO[ExitCode] = for { _ &lt;- program.handleErrorWith(_ =&gt; IO.trace.flatMap(_.printFiberTrace(options))) } yield ExitCode.Success }"
    } ,    
    {
      "title": "Home",
      "url": "/cats-effect/",
      "content": "Cats Effect Cats Effect is a high-performance, asynchronous, composable framework for building real-world applications in a purely functional style within the Typelevel ecosystem. It provides a concrete tool, known as “the IO monad”, for capturing and controlling actions, often referred to as “effects”, that your program wishes to perform within a resource-safe, typed context with seamless support for concurrency and coordination. These effects may be asynchronous (callback-driven) or synchronous (directly returning values); they may return within microseconds or run infinitely. Even more importantly, Cats Effect defines a set of typeclasses which define what it means to be a purely functional runtime system. These abstractions power a thriving ecosystem consisting of streaming frameworks, JDBC database layers, HTTP servers and clients, asynchronous clients for systems like Redis and MongoDB, and so much more! Additionally, you can leverage these abstractions within your own application to unlock powerful capabilities with little-or-no code changes, for example solving problems such as dependency injection, multiple error channels, shared state across modules, tracing, and more. Usage Versions of Cats Effect: Stable: 2.3.1 See compatibility and versioning for more information on our compatibility and semantic versioning policies. libraryDependencies += \"org.typelevel\" %% \"cats-effect\" % \"2.3.1\" Cats Effect relies on improved type inference and needs partial unification enabled as described in the Cats Getting Started documentation. If your project uses ScalaJS, replace the double-% with a triple. Note that cats-effect has an upstream dependency on cats-core version 2.x. Cross-builds are available for Scala 2.12.x, 2.13.x, 3.0.x, with ScalaJS builds targeting 1.x. The most current snapshot (or major release) can be found in the maven badge at the top of this readme. If you are a very brave sort, you are free to depend on snapshots; they are stable versions, as they are derived from the git hash rather than an unstable -SNAPSHOT suffix, but they do not come with any particular confidence or compatibility guarantees. Please see this document for information on how to cryptographically verify the integrity of cats-effect releases. You should absolutely be doing this! It takes five minutes and eliminates the need to trust a third-party with your classpath. Laws The cats-effect-laws artifact provides Discipline-style laws for the Sync, Async, Concurrent, Effect and ConcurrentEffect typeclasses (LiftIO is lawless, but highly parametric). It is relatively easy to use these laws to test your own implementations of these typeclasses. Take a look here for more. libraryDependencies += \"org.typelevel\" %% \"cats-effect-laws\" % \"2.3.1\" % \"test\" These laws are compatible with both Specs2 and ScalaTest. Documentation Links: Website: typelevel.org/cats-effect/ ScalaDoc: typelevel.org/cats-effect/api/ Related Cats links (the core): Website: typelevel.org/cats/ ScalaDoc: typelevel.org/cats/api/ Libraries These are some well known libraries that depend on cats-effect: Project Description Ciris Lightweight, extensible, and validated configuration loading in Scala Doobie A principled JDBC layer for Scala Eff Extensible Effects for Scala Fs2 Functional Streams for Scala (Streaming I/O library) Finch Scala combinator API for building Finagle HTTP services Http4s Typeful, functional, streaming HTTP for Scala Monix / Monix BIO Asynchronous, Reactive Programming for Scala and ScalaJS Pure Config A boilerplate-free library for loading configuration files Scala Cache A facade for the most popular cache implementations for Scala Sttp The Scala HTTP client you always wanted Related Projects These are some of the projects that provide high-level functions on top of cats-effect: Project Description Cats Retry A library for retrying actions that can fail Console4cats Console I/O for Cats Effect Fuuid Functional UUID’s Linebacker Thread Pool Management for Scala: Enabling functional blocking where needed Log4cats Functional Logging Cats STM Software Transactional Memory for Cats Effect Mau A tiny library for an auto polling Ref Odin Fast &amp; Functional logger with own logging backend cats-effect-testing Experimental integration between cats-effect and testing frameworks graphite4s lightweight graphite client Development We use the standard pull request driven github workflow. Pull requests are always welcome, even if it’s for something as minor as a whitespace tweak! If you’re a maintainer, you are expected to do your work in pull requests, rather than pushing directly to the main branch. Ideally, someone other than yourself will merge your PR. However, if you’ve received at least one explicit 👍 from another maintainer (or significant volume of 👍 from the general Cats community), you may merge your own PR in the interest of moving forward with important efforts. Please don’t abuse this policy. Do not rebase commits that have been PR’d! That history doesn’t belong to you anymore, and it is not yours to rewrite. This goes for maintainers and contributors alike. Rebasing locally is completely fine (and encouraged), since linear history is pretty and checkpoint commits are not. Just don’t rebase something that’s already out there unless you’ve explicitly marked it as a work in progress (e.g. [WIP]) in some clear and unambiguous way. cats-effect is a Typelevel project. This means we embrace pure, typeful, functional programming, and provide a safe and friendly environment for teaching, learning, and contributing as described in the Code of Conduct. Contributing documentation The sources for the cats-effect microsite can be found in site/src/main/mdoc. The menu structure is in site/src/main/resources/microsite/data/menu.yml. You can build the microsite with sbt microsite/makeMicrosite. To preview your changes you need to have jekyll installed. This depends on your platform, but assuming you have ruby installed it could be as simple as gem install jekyll jekyll-relative-links sass. Alternatively, you can use the provided Gemfile under site to install jekyll and the required plugins. Start a local server by navigating to site/target/site, then run jekyll serve -b /cats-effect. Finally point your browser at http://localhost:4000/cats-effect/. Any changes should be picked up immediately when you re-run sbt microsite/makeMicrosite. Tool Sponsorship Development of Cats Effect is generously supported in part by YourKit through the use of their excellent Java profiler. License Copyright (c) 2017-2021 The Typelevel Cats-effect Project Developers Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
    } ,    
    {
      "title": "Data Types",
      "url": "/cats-effect/datatypes/",
      "content": "Data Types IO A data type for encoding synchronous and asynchronous side effects as pure values SyncIO A data type for encoding synchronous side effects as pure values Fiber A pure result of a Concurrent data type being started concurrently and that can be either joined or canceled def cancel: F[Unit] def join: F[A] Resource A resource management data type that complements the Bracket typeclass Clock Provides the current time, used for time measurements and getting the current clock def realTime(unit: TimeUnit): F[Long] def monotonic(unit: TimeUnit): F[Long] ContextShift A pure equivalent of an ExecutionContext. Provides support for cooperative yielding and shifting execution, e.g. to execute blocking code on a dedicated execution context. Instance for IO is required by Concurrent[IO] def shift: F[Unit] def evalOn[A](ec: ExecutionContext)(f: F[A]): F[A] Timer A pure scheduler. Provides the ability to get the current time and delay the execution of a task with a specified time duration. Instance for IO is required by IO.sleep, timeout, etc. def clock: Clock[F] def sleep(duration: FiniteDuration): F[Unit]"
    } ,    
    {
      "title": "Type Classes",
      "url": "/cats-effect/typeclasses/",
      "content": "Type Classes The following graphic represents the current hierarchy of Cats Effect: MonadError belongs in the Cats project whereas the rest of the typeclasses belong in Cats Effect. On the side menu, you’ll find more information about each of them, including examples. Cheat sheet Bracket Can safely acquire and release resources def bracket[A, B](acquire: F[A])(use: A =&gt; F[B]) (release: A =&gt; F[Unit]): F[B] LiftIO Can convert any given IO[A] into F[A]. Useful for defining parametric signatures and composing monad transformer stacks def liftIO[A](ioa: IO[A]): F[A] Sync Can suspend(describe) synchronous side-effecting code in F. But can’t evaluate(run) it def delay[A](thunk: =&gt; A): F[A] Async Can suspend synchronous/asynchronous side-effecting code in F. Can describe computations, that may be executed independent of the main program flow (on another thread or machine) def async[A](k: (Either[Throwable, A] =&gt; Unit) =&gt; Unit): F[A] Concurrent Can concurrently start or cancel the side-effecting code in F def start[A](fa: F[A]): F[Fiber[F, A]] def race[A, B](fa: F[A], fb: F[B]): F[Either[A, B]] def cancelable[A](k: (Either[Throwable, A] =&gt; Unit) =&gt; CancelToken[F]): F[A] trait Fiber[F[_], A] { def cancel: F[Unit] def join: F[A] } Effect Allows lazy and potentially asynchronous evaluation of side-effecting code in F def runAsync[A](fa: F[A])(cb: Either[Throwable, A] =&gt; IO[Unit]): SyncIO[Unit] ConcurrentEffect Allows cancelable and concurrent evaluation of side-effecting code in F def runCancelable[A](fa: F[A])(cb: Either[Throwable, A] =&gt; IO[Unit]): SyncIO[CancelToken[F]]"
    } ,    
    {
      "title": "Testing with cats-effect",
      "url": "/cats-effect/testing/",
      "content": "Compatible libraries Relatively few libraries support cats-effect directly at this time. However, most (if not all) popular testing frameworks have libraries offering some level of integration cats-effect-testing: Supports Scalatest, Specs2, munit, minitest, µTest, and scalacheck distage-testkit: Supported natively @Daenyth’s IOSpec gist for scalatest (library pending) munit-cats-effect weaver-test: Supported natively Property-based Testing Scalacheck Scalacheck primarily supports properties in the shape A =&gt; Assertion. To support writing effectful properties with the shape A =&gt; F[Assertion], you can use one of these tools: scalacheck-effect cats-effect-testing - though note that this doesn’t support “true” async, as it does block threads under the hood. You might also want to use cats-scalacheck, which provides instances of cats-core typeclasses. Best practices Avoid using unsafe* methods in tests, the same as you’d avoid them in “main” code. Writing tests to be structured the same way as “normal” code results in tests that are less likely to be flaky, act as executable documentation, and remain easy to read. Use a compatible framework’s support for writing IO[Assertion] style tests. Testing concurrency In many test cases, TestContext should be used as an ExecutionContext, ContextShift, and Timer. This gives you very precise control over the sequencing and scheduling of evaluation, making it possible to deterministically replicate certain race conditions or even identify timing-related bugs without having to rely on non-deterministic thread ordering. TestContext also gives you control over time (as exposed by Timer and Clock), which makes it easy to write unit tests for time-related semantics without having to rely on sleeps or other hacks. To simulate time passing in a test, use Timer[F].sleep(duration), which will defer to TestContext. Be aware though that TestContext is a very artificial environment, and it can in turn mask bugs that a realistic executor would uncover. The most comprehensive test suites will often use a balance of both TestContext and a more real-world thread pool implementation. Some reference material on this approach: TestContext api documentation (includes examples and motivation) Time Traveling in Tests with Cats-Effect, by Krzysztof Ciesielski Managing shared resources Sometimes you’ll want to write a test that acquires a Resource before the suite and releases it after. For example, spinning up a database. With the Weaver test framework, this is supported using cats-effect Resource out of the box. distage-testkit test framework extends the usefulness of Resource further, allowing to designate resources to be acquired only once globally for all test suites or for a subset of test suites. (Resource Reuse - Memoization) For other test frameworks that use imperative “hook”-style methods (such as scalatest’s BeforeAndAfterAll mixin), you can use allocated class Database { def close(): Unit = ??? } object Database { def resource: Resource[IO, Database] = Resource.make(IO(new Database))(d =&gt; IO(d.close())) } class TestSuite { private var _database: Option[(Database, IO[Unit])] = None private def database: Database = _database.getOrElse(sys.error(\"not currently alive!\"))._1 def beforeAll: Unit = { _database = Some(Database.resource.allocated.unsafeRunSync()) () } def afterAll: Unit = { _database.foreach(_._2.unsafeRunSync()) _database = None } /* tests using `database` */ def test = { assert(database != null) } }"
    } ,    
    {
      "title": "IO",
      "url": "/cats-effect/datatypes/io.html",
      "content": "A data type for encoding side effects as pure values, capable of expressing both synchronous and asynchronous computations. Introduction A value of type IO[A] is a computation which, when evaluated, can perform effects before returning a value of type A. IO values are pure, immutable values and thus preserves referential transparency, being usable in functional programming. An IO is a data structure that represents just a description of a side effectful computation. IO can describe synchronous or asynchronous computations that: on evaluation yield exactly one result can end in either success or failure and in case of failure flatMap chains get short-circuited (IO implementing the algebra of MonadError) can be canceled, but note this capability relies on the user to provide cancellation logic Effects described via this abstraction are not evaluated until the “end of the world”, which is to say, when one of the “unsafe” methods are used. Effectful results are not memoized, meaning that memory overhead is minimal (and no leaks), and also that a single effect may be run multiple times in a referentially-transparent manner. For example: import cats.effect.IO val ioa = IO { println(\"hey!\") } val program: IO[Unit] = for { _ &lt;- ioa _ &lt;- ioa } yield () program.unsafeRunSync() //=&gt; hey! //=&gt; hey! () The above example prints “hey!” twice, as the effect re-runs each time it is sequenced in the monadic chain. On Referential Transparency and Lazy Evaluation IO can suspend side effects and is thus a lazily evaluated data type, being many times compared with Future from the standard library and to understand the landscape in terms of the evaluation model (in Scala), consider this classification:   Eager Lazy Synchronous A () =&gt; A     Eval[A] Asynchronous (A =&gt; Unit) =&gt; Unit () =&gt; (A =&gt; Unit) =&gt; Unit   Future[A] IO[A] In comparison with Scala’s Future, the IO data type preserves referential transparency even when dealing with side effects and is lazily evaluated. In an eager language like Scala, this is the difference between a result and the function producing it. Similar with Future, with IO you can reason about the results of asynchronous processes, but due to its purity and laziness IO can be thought of as a specification (to be evaluated at the “end of the world”), yielding more control over the evaluation model and being more predictable, for example when dealing with sequencing vs parallelism, when composing multiple IOs or when dealing with failure. Note laziness goes hand in hand with referential transparency. Consider this example: for { _ &lt;- addToGauge(32) _ &lt;- addToGauge(32) } yield () If we have referential transparency, we can rewrite that example as: val task = addToGauge(32) for { _ &lt;- task _ &lt;- task } yield () This doesn’t work with Future, but works with IO and this ability is essential for functional programming. Stack Safety IO is trampolined in its flatMap evaluation. This means that you can safely call flatMap in a recursive function of arbitrary depth, without fear of blowing the stack: def fib(n: Int, a: Long = 0, b: Long = 1): IO[Long] = IO(a + b).flatMap { b2 =&gt; if (n &gt; 0) fib(n - 1, b, b2) else IO.pure(a) } IO implements all the typeclasses shown in the hierarchy. Therefore all those operations are available for IO, in addition to some others. Describing Effects IO is a potent abstraction that can efficiently describe multiple kinds of effects: Pure Values — IO.pure &amp; IO.unit You can lift pure values into IO, yielding IO values that are “already evaluated”, the following function being defined on IO’s companion: def pure[A](a: A): IO[A] = ??? Note that the given parameter is passed by value, not by name. For example we can lift a number (pure value) into IO and compose it with another IO that wraps a side a effect in a safe manner, as nothing is going to be executed: IO.pure(25).flatMap(n =&gt; IO(println(s\"Number is: $n\"))) It should be obvious that IO.pure cannot suspend side effects, because IO.pure is eagerly evaluated, with the given parameter being passed by value, so don’t do this: IO.pure(println(\"THIS IS WRONG!\")) In this case the println will trigger a side effect that is not suspended in IO and given this code that probably is not our intention. IO.unit is simply an alias for IO.pure(()), being a reusable reference that you can use when an IO[Unit] value is required, but you don’t need to trigger any other side effects: val unit: IO[Unit] = IO.pure(()) Given IO[Unit] is so prevalent in Scala code, the Unit type itself being meant to signal completion of side effectful routines, this proves useful as a shortcut and as an optimization, since the same reference is returned. Synchronous Effects — IO.apply It’s probably the most used builder and the equivalent of Sync[IO].delay, describing IO operations that can be evaluated immediately, on the current thread and call-stack: def apply[A](body: =&gt; A): IO[A] = ??? Note the given parameter is passed ‘‘by name’’, its execution being “suspended” in the IO context. An example would be reading / writing from / to the console, which on top of the JVM uses blocking I/O, so their execution is immediate: def putStrLn(value: String) = IO(println(value)) val readLn = IO(scala.io.StdIn.readLine()) And then we can use that to model interactions with the console in a purely functional way: for { _ &lt;- putStrLn(\"What's your name?\") n &lt;- readLn _ &lt;- putStrLn(s\"Hello, $n!\") } yield () Asynchronous Effects — IO.async &amp; IO.cancelable IO can describe asynchronous processes via the IO.async and IO.cancelable builders. IO.async is the operation that complies with the laws of Async#async (see Async) and can describe simple asynchronous processes that cannot be canceled, its signature being: def async[A](k: (Either[Throwable, A] =&gt; Unit) =&gt; Unit): IO[A] = ??? The provided registration function injects a callback that you can use to signal either successful results (with Right(a)), or failures (with Left(error)). Users can trigger whatever asynchronous side effects are required, then use the injected callback to signal completion. For example, you don’t need to convert Scala’s Future, because you already have a conversion operation defined in IO.fromFuture, however the code for converting a Future would be straightforward: import scala.concurrent.{Future, ExecutionContext} import scala.util.{Success, Failure} def convert[A](fa: =&gt; Future[A])(implicit ec: ExecutionContext): IO[A] = IO.async { cb =&gt; // This triggers evaluation of the by-name param and of onComplete, // so it's OK to have side effects in this callback fa.onComplete { case Success(a) =&gt; cb(Right(a)) case Failure(e) =&gt; cb(Left(e)) } } Cancelable Processes For building cancelable IO tasks you need to use the IO.cancelable builder, this being compliant with Concurrent#cancelable (see Concurrent) and has this signature: def cancelable[A](k: (Either[Throwable, A] =&gt; Unit) =&gt; IO[Unit]): IO[A] = ??? So it is similar with IO.async, but in that registration function the user is expected to provide an IO[Unit] that captures the required cancellation logic. Important: cancellation is the ability to interrupt an IO task before completion, possibly releasing any acquired resources, useful in race conditions to prevent leaks. As example suppose we want to describe a sleep operation that depends on Java’s ScheduledExecutorService, delaying a tick for a certain time duration: import java.util.concurrent.ScheduledExecutorService import scala.concurrent.duration._ def delayedTick(d: FiniteDuration) (implicit sc: ScheduledExecutorService): IO[Unit] = { IO.cancelable { cb =&gt; val r = new Runnable { def run() = cb(Right(())) } val f = sc.schedule(r, d.length, d.unit) // Returning the cancellation token needed to cancel // the scheduling and release resources early IO(f.cancel(false)).void } } Note this delayed tick is already described by IO.sleep (via Timer), so you don’t need to do it. More on dealing with ‘‘cancellation’’ below. IO.never IO.never represents a non-terminating IO defined in terms of async, useful as shortcut and as a reusable reference: val never: IO[Nothing] = IO.async(_ =&gt; ()) This is useful in order to use non-termination in certain cases, like race conditions. For example, given IO.race, we have these equivalences: IO.race(lh, IO.never) &lt;-&gt; lh.map(Left(_)) IO.race(IO.never, rh) &lt;-&gt; rh.map(Right(_)) Deferred Execution — IO.suspend The IO.suspend builder has this equivalence: IO.suspend(f) &lt;-&gt; IO(f).flatten So it is useful for suspending effects, but that defers the completion of the returned IO to some other reference. It’s also useful for modeling stack safe, tail recursive loops: import cats.effect.IO def fib(n: Int, a: Long, b: Long): IO[Long] = IO.suspend { if (n &gt; 0) fib(n - 1, b, a + b) else IO.pure(a) } Normally a function like this would eventually yield a stack overflow error on top of the JVM. By using IO.suspend and doing all of those cycles using IO’s run-loop, its evaluation is lazy and it’s going to use constant memory. This would work with flatMap as well, of course, suspend being just nicer in this example. We could describe this function using Scala’s @tailrec mechanism, however by using IO we can also preserve fairness by inserting asynchronous boundaries: import cats.effect._ def fib(n: Int, a: Long, b: Long)(implicit cs: ContextShift[IO]): IO[Long] = IO.suspend { if (n == 0) IO.pure(a) else { val next = fib(n - 1, b, a + b) // Every 100 cycles, introduce a logical thread fork if (n % 100 == 0) cs.shift *&gt; next else next } } And now we have something more interesting than a @tailrec loop. As can be seen, IO allows very precise control over the evaluation. Concurrency and Cancellation IO can describe interruptible asynchronous processes. As an implementation detail: not all IO tasks are cancelable. Cancellation status is only checked after asynchronous boundaries. It can be achieved in the following way: Building it with IO.cancelable, IO.async, IO.asyncF or IO.bracket Using IO.cancelBoundary or IO.shift Note that the second point is the consequence of the first one and anything that involves those operations is also possible to cancel. It includes, but is not limited to waiting on Mvar.take, Mvar.put and Deferred.get. We should also note that flatMap chains are only cancelable only if the chain happens after an asynchronous boundary. After an asynchronous boundary, cancellation checks are performed on every N flatMap. The value of N is hardcoded to 512. Here is an example, import cats.effect.{ContextShift, IO} import scala.concurrent.ExecutionContext implicit val contextShift: ContextShift[IO] = IO.contextShift(ExecutionContext.global) def retryUntilRight[A, B](io: IO[Either[A, B]]): IO[B] = { io.flatMap { case Right(b) =&gt; IO.pure(b) case Left(_) =&gt; retryUntilRight(io) } } // non-terminating IO that is NOT cancelable val notCancelable: IO[Int] = retryUntilRight(IO(Left(0))) // non-terminating IO that is cancelable because there is an // async boundary created by IO.shift before `flatMap` chain val cancelable: IO[Int] = IO.shift *&gt; retryUntilRight(IO(Left(0))) IO tasks that are cancelable, usually become non-terminating on cancel Also this might be a point of confusion for folks coming from Java and that expect the features of Thread.interrupt or of the old and deprecated Thread.stop: IO cancellation does NOT work like that, as thread interruption in Java is inherently unsafe, unreliable and not portable! Next subsections describe cancellation-related operations in more depth. Building cancelable IO tasks Cancelable IO tasks can be described via the IO.cancelable builder. The delayedTick example making use of the Java’s ScheduledExecutorService was already given above, but to recap: import java.util.concurrent.ScheduledExecutorService import cats.effect.IO import scala.concurrent.duration.FiniteDuration def sleep(d: FiniteDuration) (implicit sc: ScheduledExecutorService): IO[Unit] = { IO.cancelable { cb =&gt; val r = new Runnable { def run() = cb(Right(())) } val f = sc.schedule(r, d.length, d.unit) // Returning a function that can cancel our scheduling IO(f.cancel(false)).void } } Important: if you don’t specify cancellation logic for a task, then the task is NOT cancelable. So for example, using Java’s blocking I/O still: import java.io._ import cats.effect.IO import scala.concurrent.ExecutionContext import scala.util.control.NonFatal def unsafeFileToString(file: File) = { // Freaking Java :-) val in = new BufferedReader( new InputStreamReader(new FileInputStream(file), \"utf-8\")) try { // Uninterruptible loop val sb = new StringBuilder() var hasNext = true while (hasNext) { hasNext = false val line = in.readLine() if (line != null) { hasNext = true sb.append(line) } } sb.toString } finally { in.close() } } def readFile(file: File)(implicit ec: ExecutionContext) = IO.async[String] { cb =&gt; ec.execute(() =&gt; { try { // Signal completion cb(Right(unsafeFileToString(file))) } catch { case NonFatal(e) =&gt; cb(Left(e)) } }) } This is obviously not cancelable and there’s no magic that the IO implementation does to make that loop cancelable. No, we are not going to use Java’s Thread.interrupt, because that would be unsafe and unreliable and besides, whatever the IO does has to be portable between platforms. But there’s a lot of flexibility in what can be done, including here. We could simply introduce a variable that changes to false, to be observed in that while loop: import java.io.File import java.util.concurrent.atomic.AtomicBoolean import cats.effect.IO import scala.concurrent.ExecutionContext import scala.io.Source import scala.util.control.NonFatal def unsafeFileToString(file: File, isActive: AtomicBoolean) = { val sc = new StringBuilder val linesIterator = Source.fromFile(file).getLines() var hasNext = true while (hasNext &amp;&amp; isActive.get) { sc.append(linesIterator.next()) hasNext = linesIterator.hasNext } sc.toString } def readFile(file: File)(implicit ec: ExecutionContext) = IO.cancelable[String] { cb =&gt; val isActive = new AtomicBoolean(true) ec.execute(() =&gt; { try { // Signal completion cb(Right(unsafeFileToString(file, isActive))) } catch { case NonFatal(e) =&gt; cb(Left(e)) } }) // On cancel, signal it IO(isActive.set(false)).void } Gotcha: Cancellation is a Concurrent Action! This is not always obvious, not from the above examples, but you might be tempted to do something like this: import java.io._ import cats.effect.IO import scala.concurrent.ExecutionContext import scala.util.control.NonFatal def readLine(in: BufferedReader)(implicit ec: ExecutionContext) = IO.cancelable[String] { cb =&gt; ec.execute(() =&gt; cb( try Right(in.readLine()) catch { case NonFatal(e) =&gt; Left(e) })) // Cancellation logic is not thread-safe! IO(in.close()).void } An operation like this might be useful in streaming abstractions that stream I/O chunks via IO (via libraries like FS2, Monix, or others). But the described operation is incorrect, because in.close() is concurrent with in.readLine, which can lead to thrown exceptions and in many cases it can lead to data corruption. This is a big no-no. We want to interrupt whatever it is that the IO is doing, but not at the cost of data corruption. Therefore the user needs to handle thread safety concerns. So here’s one way of doing it: import java.io._ import java.util.concurrent.atomic.AtomicBoolean import cats.effect.IO import scala.util.control.NonFatal import scala.concurrent.ExecutionContext def readLine(in: BufferedReader)(implicit ec: ExecutionContext) = IO.cancelable[String] { cb =&gt; val isActive = new AtomicBoolean(true) ec.execute { () =&gt; if (isActive.getAndSet(false)) { try cb(Right(in.readLine())) catch { case NonFatal(e) =&gt; cb(Left(e)) } } // Note there's no else; if cancellation was executed // then we don't call the callback; task becoming // non-terminating ;-) } // Cancellation logic IO { // Thread-safe gate if (isActive.getAndSet(false)) in.close() }.void } In this example it is the cancellation logic itself that calls in.close(), but the call is safe due to the thread-safe guard that we’re creating by usage of an atomic getAndSet. This is using an AtomicBoolean for thread-safety, but don’t shy away from using intrinsic locks / mutexes via synchronize blocks or whatever else concurrency primitives the JVM provides, whatever is needed in these side effectful functions. And don’t worry, this is usually needed only in IO.cancelable, IO.async or IO.apply, as these builders represents the FFI for interacting with the impure world, aka the dark side, otherwise once you’re in IO’s context, you can compose concurrent tasks using higher level tools. Shared memory concurrency is unfortunately both the blessing and the curse of working with kernel threads. Not a big problem on N:1 platforms like JavaScript, but there you don’t get in-process CPU parallelism either. Such is life, a big trail of tradeoffs. Concurrent start + cancel You can use IO as a green-threads system, with the “fork” operation being available via IO#start, the operation that’s compliant with Concurrent#start. This is a method with the following signature: def start: IO[Fiber[IO, A]] Returned is a Fiber. You can think of fibers as being lightweight threads, a fiber being the pure and light equivalent of a thread that can be either joined (via join) or interrupted (via cancel). Example: import cats.effect.{ContextShift, IO} import scala.concurrent.ExecutionContext // Needed for IO.start to do a logical thread fork implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global) val launchMissiles: IO[Unit] = IO.raiseError(new Exception(\"boom!\")) val runToBunker = IO(println(\"To the bunker!!!\")) for { fiber &lt;- launchMissiles.start _ &lt;- runToBunker.handleErrorWith { error =&gt; // Retreat failed, cancel launch (maybe we should // have retreated to our bunker before the launch?) fiber.cancel *&gt; IO.raiseError(error) } aftermath &lt;- fiber.join } yield aftermath start is defined on IO with overloads accepting an implicit ContextShift or an explicit ExecutionContext, but it’s also available via the Concurrent type class. To get an instance of Concurrent[IO], you need a ContextShift[IO] in implicit scope. runCancelable &amp; unsafeRunCancelable The above is the pure cancel, accessible via Fiber. However the second way to access cancellation token and thus interrupt tasks is via runCancelable (the pure version) and unsafeRunCancelable (the unsafe version). Example relying on the side-effecting unsafeRunCancelable and note this kind of code is impure and should be used with care: import cats.effect.IO import scala.concurrent.ExecutionContext import scala.concurrent.duration._ // Needed for `sleep` implicit val timer = IO.timer(ExecutionContext.global) // Delayed println val io: IO[Unit] = IO.sleep(10.seconds) *&gt; IO(println(\"Hello!\")) val cancel: IO[Unit] = io.unsafeRunCancelable(r =&gt; println(s\"Done: $r\")) // ... if a race condition happens, we can cancel it, // thus canceling the scheduling of `IO.sleep` cancel.unsafeRunSync() The runCancelable alternative is the operation that’s compliant with the laws of ConcurrentEffect. Same idea, only the actual execution is suspended in SyncIO: import cats.effect.SyncIO import cats.syntax.flatMap._ val pureResult: SyncIO[IO[Unit]] = io.runCancelable { r =&gt; IO(println(s\"Done: $r\")) } // On evaluation, this will first execute the source, then it // will cancel it, because it makes perfect sense :-) pureResult.toIO.flatten uncancelable marker Given a cancelable IO, we can turn it into an IO that cannot be canceled: import cats.effect.IO import scala.concurrent.ExecutionContext import scala.concurrent.duration._ // Needed for `sleep` implicit val timer = IO.timer(ExecutionContext.global) // Our reference from above val io: IO[Unit] = IO.sleep(10.seconds) *&gt; IO(println(\"Hello!\")) // This IO can't be canceled, even if we try io.uncancelable Sometimes you need to ensure that an IO’s execution is atomic, or in other words, either all of it executes, or none of it. And this is what this operation does — cancelable IOs are by definition not atomic and in certain cases we need to make them atomic. This law is compliant with the laws of Concurrent#uncancelable (see Concurrent). IO.cancelBoundary Returns a cancelable boundary — an IO task that checks for the cancellation status of the run-loop and does not allow for the bind continuation to keep executing in case cancellation happened. This operation is very similar to IO.shift, as it can be dropped in flatMap chains in order to make such long loops cancelable: import cats.effect.IO def fib(n: Int, a: Long, b: Long): IO[Long] = IO.suspend { if (n &lt;= 0) IO.pure(a) else { val next = fib(n - 1, b, a + b) // Every 100-th cycle check cancellation status if (n % 100 == 0) IO.cancelBoundary *&gt; next else next } } As mentioned at the very beginning of this section, fairness needs to be managed explicitly, the protocol being easy to follow and predictable in a WYSIWYG fashion. Comparison to IO.shift IO.cancelBoundary is essentially lighter version of IO.shift without ability to shift into different thread pool. It is lighter in the sense that it will avoid doing logical fork. Race Conditions — race &amp; racePair A race condition is a piece of logic that creates a race between two or more tasks, with the winner being signaled immediately, with the losers being usually canceled. IO provides two operations for races in its companion: // simple version def race[A, B](lh: IO[A], rh: IO[B]) (implicit cs: ContextShift[IO]): IO[Either[A, B]] // advanced version def racePair[A, B](lh: IO[A], rh: IO[B]) (implicit cs: ContextShift[IO]): IO[Either[(A, Fiber[IO, B]), (Fiber[IO, A], B)]] The simple version, IO.race, will cancel the loser immediately, whereas the second version gives you a Fiber, letting you decide what to do next. So race can be derived with racePair like so: import cats.effect.{ContextShift, IO} def race[A, B](lh: IO[A], rh: IO[B]) (implicit cs: ContextShift[IO]): IO[Either[A, B]] = { IO.racePair(lh, rh).flatMap { case Left((a, fiber)) =&gt; fiber.cancel.map(_ =&gt; Left(a)) case Right((fiber, b)) =&gt; fiber.cancel.map(_ =&gt; Right(b)) } } Using race we could implement a “timeout” operation: import cats.effect.{ContextShift, Timer, IO} import scala.concurrent.CancellationException import scala.concurrent.duration.FiniteDuration def timeoutTo[A](fa: IO[A], after: FiniteDuration, fallback: IO[A]) (implicit timer: Timer[IO], cs: ContextShift[IO]): IO[A] = { IO.race(fa, timer.sleep(after)).flatMap { case Left(a) =&gt; IO.pure(a) case Right(_) =&gt; fallback } } def timeout[A](fa: IO[A], after: FiniteDuration) (implicit timer: Timer[IO], cs: ContextShift[IO]): IO[A] = { val error = new CancellationException(after.toString) timeoutTo(fa, after, IO.raiseError(error)) } See Parallelism section above for how to obtain a Timer[IO] Comparison with Haskell’s “async interruption” Haskell treats interruption with what they call “asynchronous exceptions”, providing the ability to interrupt a running task by throwing an exception from another thread (concurrently). For cats.effect, for the “cancel” action, what happens is that whatever you specify in the IO.cancelable builder gets executed. And depending on the implementation of an IO.cancelable task, it can become non-terminating. If we’d need to describe our cancel operation with an impure signature, it would be: () =&gt; Unit By comparison Haskell (and possibly the upcoming Scalaz 8 IO), sends an error, a Throwable on interruption and canceled tasks get completed with that Throwable. Their impure cancel is: Throwable =&gt; Unit Throwable =&gt; Unit allows the task’s logic to know the cancellation reason, however cancellation is about cutting the connection to the producer, closing all resources as soon as possible, because you’re no longer interested in the result, due to some race condition that happened. Throwable =&gt; Unit is also a little confusing, being too broad in scope. Users might be tricked into sending messages back to the producer via this channel, in order to steer it, to change its outcome - however cancellation is cancellation, we’re doing it for the purpose of releasing resources and the implementation of race conditions will end up closing the connection, disallowing the canceled task to send anything downstream. Therefore it’s confusing for the user and the only practical use is to release resources differently, based on the received error. But that’s not a use-case that’s worth pursuing, given the increase in complexity. Safe Resource Acquisition and Release Status Quo In mainstream imperative languages you usually have try / finally statements at disposal for acquisition and safe release of resources. Pattern goes like this: import java.io._ def javaReadFirstLine(file: File): String = { val in = new BufferedReader(new FileReader(file)) try { in.readLine() } finally { in.close() } } It does have problems like: this statement is obviously meant for side-effectful computations and can’t be used by FP abstractions it’s only meant for synchronous execution, so we can’t use it when working with abstractions capable of asynchrony (e.g. IO, Task, Future) finally executes regardless of the exception type, indiscriminately, so if you get an out of memory error it still tries to close the file handle, unnecessarily delaying a process crash if the body of try throws an exception, then followed by the body of finally also throwing an exception, then the exception of finally gets rethrown, hiding the original problem bracket Via the bracket operation we can easily describe the above: import java.io._ import cats.effect.IO def readFirstLine(file: File): IO[String] = IO(new BufferedReader(new FileReader(file))).bracket { in =&gt; // Usage (the try block) IO(in.readLine()) } { in =&gt; // Releasing the reader (the finally block) IO(in.close()).void } Notes: this is pure, so it can be used for FP this works with asynchronous IO actions the release action will happen regardless of the exit status of the use action, so it will execute for successful completion, for thrown errors or for canceled execution if the use action throws an error and then the release action throws an error as well, the reported error will be that of use, whereas the error thrown by release will just get logged (via System.err) Of special consideration is that bracket calls the release action on cancellation as well. Consider this sample: import java.io._ import cats.effect.{ContextShift, IO} import scala.concurrent.ExecutionContext implicit val contextShift: ContextShift[IO] = IO.contextShift(ExecutionContext.global) def readFile(file: File): IO[String] = { // Opens file with an asynchronous boundary before it, // ensuring that processing doesn't block the \"current thread\" val acquire = IO.shift *&gt; IO(new BufferedReader(new FileReader(file))) acquire.bracket { in =&gt; // Usage (the try block) IO { // Ugly, low-level Java code warning! val content = new StringBuilder() var line: String = null do { line = in.readLine() if (line != null) content.append(line) } while (line != null) content.toString() } } { in =&gt; // Releasing the reader (the finally block) // This is problematic if the resulting `IO` can get // canceled, because it can lead to data corruption IO(in.close()).void } } That loop can be slow, we could be talking about a big file and as described in the “Concurrency and Cancellation” section, cancellation is a concurrent action with whatever goes on in use. And in this case, on top of the JVM that is capable of multi-threading, calling io.close() concurrently with that loop can lead to data corruption. Depending on use-case synchronization might be needed to prevent it: import java.io._ import cats.effect.{ContextShift, IO} import scala.concurrent.ExecutionContext implicit val contextShift: ContextShift[IO] = IO.contextShift(ExecutionContext.global) def readFile(file: File): IO[String] = { // Opens file with an asynchronous boundary before it, // ensuring that processing doesn't block the \"current thread\" val acquire = IO.shift *&gt; IO(new BufferedReader(new FileReader(file))) // Suspended execution because we are going to mutate // a shared variable IO.suspend { // Shared state meant to signal cancellation var isCanceled = false acquire.bracket { in =&gt; IO { val content = new StringBuilder() var line: String = null do { // Synchronized access to isCanceled and to the reader line = in.synchronized { if (!isCanceled) in.readLine() else null } if (line != null) content.append(line) } while (line != null) content.toString() } } { in =&gt; IO { // Synchronized access to isCanceled and to the reader in.synchronized { isCanceled = true in.close() } }.void } } } bracketCase The bracketCase operation is the generalized bracket, also receiving an ExitCase in release in order to distinguish between: successful completion completion in error cancellation Usage sample: import java.io.BufferedReader import cats.effect.IO import cats.effect.ExitCase.{Completed, Error, Canceled} def readLine(in: BufferedReader): IO[String] = IO.pure(in).bracketCase { in =&gt; IO(in.readLine()) } { case (_, Completed | Error(_)) =&gt; // Do nothing IO.unit case (in, Canceled) =&gt; IO(in.close()) } In this example we are only closing the passed resource in case cancellation occurred. As to why we’re doing this — consider that the BufferedReader reference was given to us and usually the producer of such a resource should also be in charge of releasing it. If this function would release the given BufferedReader on a successful result, then this would be a flawed implementation. Remember the age old C++ idiom of “resource acquisition is initialization (RAII)”, which says that the lifetime of a resource should be tied to the lifetime of its parent. But in case we detect cancellation, we might want to close that resource, because in the case of a cancellation event, we might not have a “run-loop” active after this IO returns its result, so there might not be anybody available to release it. Conversions There are two useful operations defined in the IO companion object to lift both a scala Future and an Either into IO. fromFuture Constructs an IO which evaluates the given Future and produces either a result or a failure. It is defined as follow: import cats.effect.IO import scala.concurrent.Future def fromFuture[A](iof: IO[Future[A]]): IO[A] = ??? Because Future eagerly evaluates, as well as because it memoizes, this function takes its parameter as an IO, which could be lazily evaluated. If this laziness is appropriately threaded back to the definition site of the Future, it ensures that the computation is fully managed by IO and thus referentially transparent. Lazy evaluation, equivalent with by-name parameters: import cats.effect.{ContextShift, IO} import scala.concurrent.Future import scala.concurrent.ExecutionContext import ExecutionContext.Implicits.global implicit val contextShift: ContextShift[IO] = IO.contextShift(ExecutionContext.global) IO.fromFuture(IO { Future(println(\"I come from the Future!\")) }) Eager evaluation: val f = Future.successful(\"I come from the Future!\") IO.fromFuture(IO.pure(f)) fromEither Lifts an Either[Throwable, A] into the IO[A] context raising the throwable if it exists. import cats.effect.IO def fromEither[A](e: Either[Throwable, A]): IO[A] = e.fold(IO.raiseError, IO.pure) Error Handling Since there is an instance of MonadError[IO, Throwable] available in Cats Effect, all the error handling is done through it. This means you can use all the operations available for MonadError and thus for ApplicativeError on IO as long as the error type is a Throwable. Operations such as raiseError, attempt, handleErrorWith, recoverWith, etc. Just make sure you have the syntax import in scope such as cats.syntax.all._. raiseError Constructs an IO which sequences the specified exception. import cats.effect.IO val boom: IO[Unit] = IO.raiseError(new Exception(\"boom\")) boom.unsafeRunSync() attempt Materializes any sequenced exceptions into value space, where they may be handled. This is analogous to the catch clause in try/catch, being the inverse of IO.raiseError. Example: import cats.effect.IO val boom: IO[Unit] = IO.raiseError(new Exception(\"boom\")) boom.attempt.unsafeRunSync() Look at the MonadError typeclass for more. Example: Retrying with Exponential Backoff With IO you can easily model a loop that retries evaluation until success or some other condition is met. For example here’s a way to implement retries with exponential back-off: import cats.effect.{IO, Timer} import scala.concurrent.duration._ def retryWithBackoff[A](ioa: IO[A], initialDelay: FiniteDuration, maxRetries: Int) (implicit timer: Timer[IO]): IO[A] = { ioa.handleErrorWith { error =&gt; if (maxRetries &gt; 0) IO.sleep(initialDelay) *&gt; retryWithBackoff(ioa, initialDelay * 2, maxRetries - 1) else IO.raiseError(error) } } Thread Shifting IO provides a function shift to give you more control over the execution of your operations. shift Note there are 2 overloads of the IO.shift function: One that takes a ContextShift that manages the thread-pool used to trigger async boundaries. Another that takes a Scala ExecutionContext as the thread-pool. Please use the former by default and use the latter only for fine-grained control over the thread pool in use. By default, Cats Effect provides an instance of ContextShift[IO] that manages thread-pools, but only inside an implementation of IOApp. Custom instances of ContextShift[IO] can be created using an ExecutionContext: import cats.effect.{ContextShift, IO} import scala.concurrent.ExecutionContext.Implicits.global implicit val contextShift: ContextShift[IO] = IO.contextShift(global) We can introduce an asynchronous boundary in the flatMap chain before a certain task: val task = IO(println(\"task\")) IO.shift(contextShift).flatMap(_ =&gt; task) Note that the ContextShift value is taken implicitly from the context so you can just do this: IO.shift.flatMap(_ =&gt; task) Or using Cats syntax: IO.shift *&gt; task // equivalent to implicitly[ContextShift[IO]].shift *&gt; task Or we can specify an asynchronous boundary “after” the evaluation of a certain task: task.flatMap(a =&gt; IO.shift.map(_ =&gt; a)) Or using Cats syntax: task &lt;* IO.shift // equivalent to task &lt;* implicitly[ContextShift[IO]].shift Example of where this might be useful: import java.util.concurrent.Executors import cats.effect.IO import scala.concurrent.ExecutionContext val cachedThreadPool = Executors.newCachedThreadPool() val BlockingFileIO = ExecutionContext.fromExecutor(cachedThreadPool) implicit val Main = ExecutionContext.global val ioa: IO[Unit] = for { _ &lt;- IO(println(\"Enter your name: \")) _ &lt;- IO.shift(BlockingFileIO) name &lt;- IO(scala.io.StdIn.readLine()) _ &lt;- IO.shift(Main) _ &lt;- IO(println(s\"Welcome $name!\")) _ &lt;- IO(cachedThreadPool.shutdown()) } yield () We start by asking the user to enter its name and next we thread-shift to the BlockingFileIO execution context because we expect the following action to block on the thread for a long time and we don’t want that to happen in the main thread of execution. After the expensive IO operation (readLine) gets back with a response we thread-shift back to the main execution context defined as an implicit value, and finally the program ends by showing a message in the console and shutting down a thread pool, all actions run in the main execution context. Another somewhat less common application of shift is to reset the thread stack and yield control back to the underlying pool. For example: import cats.effect.{ContextShift, IO} import scala.concurrent.ExecutionContext implicit val contextShift: ContextShift[IO] = IO.contextShift(ExecutionContext.global) lazy val doStuff = IO(println(\"stuff\")) lazy val repeat: IO[Unit] = for { _ &lt;- doStuff _ &lt;- IO.shift _ &lt;- repeat } yield () In this example, repeat is a very long running IO (infinite, in fact!) which will just hog the underlying thread resource for as long as it continues running. This can be a bit of a problem, and so we inject the IO.shift which yields control back to the underlying thread pool, giving it a chance to reschedule things and provide better fairness. This shifting also “bounces” the thread stack, popping all the way back to the thread pool and effectively trampolining the remainder of the computation. Although the thread-shifting is not completely necessary, it might help in some cases to alleviate the use of the main thread pool. Thus, this function has four important use cases: Shifting blocking actions off of the main compute pool. Defensively re-shifting asynchronous continuations back to the main compute pool. Yielding control to some underlying pool for fairness reasons. Preventing an overflow of the call stack in the case of improperly constructed async actions. IO is trampolined for all synchronous and asynchronous joins. This means that you can safely call flatMap in a recursive function of arbitrary depth, without fear of blowing the stack. So you can do this for example: import cats.effect.IO def signal[A](a: A): IO[A] = IO.async(_(Right(a))) def loop(n: Int): IO[Int] = signal(n).flatMap { x =&gt; if (x &gt; 0) loop(n - 1) else IO.pure(0) } Parallelism Since the introduction of the Parallel typeclasss in the Cats library and its IO instance, it became possible to execute two or more given IOs in parallel. Note: all parallel operations require an implicit ContextShift[IO] in scope (see ContextShift). You have a ContextShift in scope if: via usage of IOApp that gives you a ContextShift by default the user provides a custom ContextShift, which can be created using IO.contextShift(executionContext) parMapN It has the potential to run an arbitrary number of IOs in parallel, and it allows you to apply a function to the result (as in map). It finishes processing when all the IOs are completed, either successfully or with a failure. For example: import cats.effect.{ContextShift, IO} import cats.syntax.all._ import scala.concurrent.ExecutionContext implicit val contextShift: ContextShift[IO] = IO.contextShift(ExecutionContext.global) val ioA = IO(println(\"Running ioA\")) val ioB = IO(println(\"Running ioB\")) val ioC = IO(println(\"Running ioC\")) // make sure that you have an implicit ContextShift[IO] in scope. val program = (ioA, ioB, ioC).parMapN { (_, _, _) =&gt; () } program.unsafeRunSync() //=&gt; Running ioB //=&gt; Running ioC //=&gt; Running ioA () If any of the IOs completes with a failure then the result of the whole computation will be failed, while the unfinished tasks get cancelled. Example: import cats.effect.{ContextShift, ExitCase, IO} import cats.syntax.all._ import scala.concurrent.ExecutionContext import scala.concurrent.duration._ implicit val contextShift: ContextShift[IO] = IO.contextShift(ExecutionContext.global) implicit val timer = IO.timer(ExecutionContext.global) val a = IO.raiseError[Unit](new Exception(\"boom\")) &lt;* IO(println(\"Running ioA\")) val b = (IO.sleep(1.second) *&gt; IO(println(\"Running ioB\"))) .guaranteeCase { case ExitCase.Canceled =&gt; IO(println(\"ioB was canceled!\")) case _ =&gt; IO.unit } val parFailure = (a, b).parMapN { (_, _) =&gt; () } parFailure.attempt.unsafeRunSync() //=&gt; ioB was canceled! //=&gt; java.lang.Exception: boom //=&gt; ... 43 elided () If one of the tasks fails immediately, then the other gets canceled and the computation completes immediately, so in this example the pairing via parMapN will not wait for 10 seconds before emitting the error: import cats.effect.{ContextShift, Timer, IO} import cats.syntax.all._ import scala.concurrent.ExecutionContext import scala.concurrent.duration._ implicit val contextShift: ContextShift[IO] = IO.contextShift(ExecutionContext.global) implicit val timer: Timer[IO] = IO.timer(ExecutionContext.global) val ioA = IO.sleep(10.seconds) *&gt; IO(println(\"Delayed!\")) val ioB = IO.raiseError[Unit](new Exception(\"dummy\")) (ioA, ioB).parMapN((_, _) =&gt; ()) parSequence If you have a list of IO, and you want a single IO with the result list you can use parSequence which executes the IO tasks in parallel. import cats.data.NonEmptyList import cats.effect.{ContextShift, Timer, IO} import cats.syntax.parallel._ import scala.concurrent.ExecutionContext // Needed for IO.start to do a logical thread fork implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global) implicit val timer: Timer[IO] = IO.timer(ExecutionContext.global) val anIO = IO(1) val aLotOfIOs = NonEmptyList.of(anIO, anIO) val ioOfList = aLotOfIOs.parSequence There is also cats.Traverse.sequence which does this synchronously. parTraverse If you have a list of data and a way of turning each item into an IO, but you want a single IO for the results you can use parTraverse to run the steps in parallel. import cats.data.NonEmptyList import cats.effect.{ContextShift, Timer, IO} import cats.syntax.parallel._ import scala.concurrent.ExecutionContext // Needed for IO.start to do a logical thread fork implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global) implicit val timer: Timer[IO] = IO.timer(ExecutionContext.global) val results = NonEmptyList.of(1, 2, 3).parTraverse { i =&gt; IO(i) } There is also cats.Traverse.traverse which will run each step synchronously. “Unsafe” Operations We have been using some “unsafe” operations pretty much everywhere in the previous examples but we never explained any of them, so here it goes. All of the operations prefixed with unsafe are impure functions and perform side effects (for example Haskell has unsafePerformIO). But don’t be scared by the name! You should write your programs in a monadic way using functions such as map and flatMap to compose other functions and ideally you should just call one of these unsafe operations only once, at the very end of your program. unsafeRunSync Produces the result by running the encapsulated effects as impure side effects. If any component of the computation is asynchronous, the current thread will block awaiting the results of the async computation. On JavaScript, an exception will be thrown instead to avoid generating a deadlock. By default, this blocking will be unbounded. To limit the thread block to some fixed time, use unsafeRunTimed instead. Any exceptions raised within the effect will be re-thrown during evaluation. IO(println(\"Sync!\")).unsafeRunSync() // Sync! unsafeRunAsync Passes the result of the encapsulated effects to the given callback by running them as impure side effects. Any exceptions raised within the effect will be passed to the callback in the Either. The callback will be invoked at most once. Note that it is very possible to construct an IO which never returns while still never blocking a thread, and attempting to evaluate that IO with this method will result in a situation where the callback is never invoked. IO(println(\"Async!\")).unsafeRunAsync(_ =&gt; ()) // Async! unsafeRunCancelable Evaluates the source IO, passing the result of the encapsulated effects to the given callback. Note that this has the potential to be interrupted. IO(println(\"Potentially cancelable!\")).unsafeRunCancelable(_ =&gt; ()) // Potentially cancelable! // res59: cats.effect.package.CancelToken[IO] = Suspend( // thunk = cats.effect.internals.IOConnection$Impl$$Lambda$11923/0x00000008030d7c40@5809ede6 // ) unsafeRunTimed Similar to unsafeRunSync, except with a bounded blocking duration when awaiting asynchronous results. Please note that the limit parameter does not limit the time of the total computation, but rather acts as an upper bound on any individual asynchronous block. Thus, if you pass a limit of 5 seconds to an IO consisting solely of synchronous actions, the evaluation may take considerably longer than 5 seconds! Furthermore, if you pass a limit of 5 seconds to an IO consisting of several asynchronous actions joined together, evaluation may take up to n * 5 seconds, where n is the number of joined async actions. As soon as an async blocking limit is hit, evaluation “immediately” aborts and None is returned. Please note that this function is intended for testing purposes; it should never appear in your mainline production code! It is absolutely not an appropriate function to use if you want to implement timeouts, or anything similar. If you need that sort of functionality, you should be using a streaming library (like fs2 or Monix). import scala.concurrent.duration._ IO(println(\"Timed!\")).unsafeRunTimed(5.seconds) unsafeToFuture Evaluates the effect and produces the result in a Future. This is similar to unsafeRunAsync in that it evaluates the IO as a side effect in a non-blocking fashion, but uses a Future rather than an explicit callback. This function should really only be used if interoperating with legacy code which uses Scala futures. IO(\"Gimme a Future!\").unsafeToFuture() Best Practices This section presents some best practices for working with IO: Keep Granularity It’s better to keep the granularity, so please don’t do something like this: IO { readingFile writingToDatabase sendBytesOverTcp launchMissiles } In FP we embrace reasoning about our programs and since IO is a Monad you can compose bigger programs from small ones in a for-comprehension. For example: val program = for { data &lt;- readFile _ &lt;- writeToDatabase(data) _ &lt;- sendBytesOverTcp(data) _ &lt;- launchMissiles } yield () Each step of the comprehension is a small program, and the resulting program is a composition of all those small steps, which is compositional with other programs. IO values compose. Use pure functions in map / flatMap When using map or flatMap it is not recommended to pass a side effectful function, as mapping functions should also be pure. So this should be avoided: IO.pure(123).map(n =&gt; println(s\"NOT RECOMMENDED! $n\")) This too should be avoided, because the side effect is not suspended in the returned IO value: IO.pure(123).flatMap { n =&gt; println(s\"NOT RECOMMENDED! $n\") IO.unit } The correct approach would be this: IO.pure(123).flatMap { n =&gt; // Properly suspending the side effect IO(println(s\"RECOMMENDED! $n\")) } Note that as far as the actual behavior of IO is concerned, something like IO.pure(x).map(f) is equivalent with IO(f(x)) and IO.pure(x).flatMap(f) is equivalent with IO.suspend(f(x)). But you should not rely on this behavior, because it is NOT described by the laws required by the Sync type class and those laws are the only guarantees of behavior that you get. For example the above equivalence might be broken in the future in regards to error handling. So this behavior is currently there for safety reasons, but you should regard it as an implementation detail that could change in the future. Stick with pure functions."
    } ,    
    {
      "title": "IOApp",
      "url": "/cats-effect/datatypes/ioapp.html",
      "content": "IOApp is a safe application type that describes a main which executes a cats.effect.IO, as an entry point to a pure FP program. Status Quo Currently in order to specify an entry point to a Java application, which executes a pure FP program (with side effects suspended and described by IO), you have to do something like this: import cats.effect._ import scala.concurrent.duration._ import scala.concurrent.ExecutionContext object Main { // Needed for `IO.sleep` implicit val timer: Timer[IO] = IO.timer(ExecutionContext.global) def program(args: List[String]): IO[Unit] = IO.sleep(1.second) *&gt; IO(println(s\"Hello world!. Args $args\")) def main(args: Array[String]): Unit = program(args.toList).unsafeRunSync() } That’s dirty, error prone and doesn’t work on top of JavaScript. Pure Programs You can now use cats.effect.IOApp to describe pure programs: import cats.effect._ object Main extends IOApp { def run(args: List[String]): IO[ExitCode] = args.headOption match { case Some(name) =&gt; IO(println(s\"Hello, $name.\")).as(ExitCode.Success) case None =&gt; IO(System.err.println(\"Usage: MyApp name\")).as(ExitCode(2)) } } Things to note: the command line arguments get passed as a pure List instead of an Array we use an ExitCode to specify success or an error code, the implementation handling how that is returned and thus you no longer have to deal with a side-effectful Runtime.exit call the Timer[IO] dependency is already provided by IOApp, so on top of the JVM there’s no longer a need for an implicit ExecutionContext to be in scope In terms of the behavior, the contract is currently this: if the program returns an ExitCode.Success, the main method exits and shutdown is handled by the platform — this is meant to play nice with Spark and also be consistent with Java’s behavior (e.g. non-daemon threads will block the app from exiting, unless you do something about it) if completed with an exit code different from zero, the app is exited with that as an error code (via sys.exit) if the IO terminates in error, it is printed to standard error and sys.exit is called Cancelation and Safe Resource Release The cats.effect.IO implementation is cancelable and so is IOApp. This means that when IOApp receives a SIGABORT, SIGINT or another interruption signal that can be caught, then the IO app will cancel and safely release any resources. WARNING: If you run your IOApp program from sbt, you may observe cancellation and resource releasing is not happening. This is due to sbt, by default, running programs in the same JVM as sbt, so when your program is canceled sbt avoids stopping its own JVM. To properly allow cancellation, ensure your progam is forked into its own JVM via a setting like fork := true in your sbt configuration. For example: import cats.effect.ExitCase.Canceled import cats.effect._ import scala.concurrent.duration._ object Main extends IOApp { def loop(n: Int): IO[ExitCode] = IO.suspend { if (n &lt; 10) IO.sleep(1.second) *&gt; IO(println(s\"Tick: $n\")) *&gt; loop(n + 1) else IO.pure(ExitCode.Success) } def run(args: List[String]): IO[ExitCode] = loop(0).guaranteeCase { case Canceled =&gt; IO(println(\"Interrupted: releasing and exiting!\")) case _ =&gt; IO(println(\"Normal exit!\")) } } If you run this sample, you can get two outcomes: if you leave it for 10 seconds, it will print “normal exit” and exit normally if you press Ctrl-C or do a kill $pid from the terminal, then it will immediately print “interrupted: releasing and exiting” and exit Therefore IOApp automatically installs an interruption handler for you. Why Is It Specialized for IO? IOApp doesn’t have an F[_] parameter, unlike the other data types exposed by Cats-Effect. This is because different F[_] data types have different requirements for evaluation at the end of the world. For example cats.effect.IO now needs a ContextShift[IO] in scope for working with Concurrent and thus for getting the ConcurrentEffect necessary to evaluate an IO. It also needs a Timer[IO] in scope for utilities such as IO.sleep and timeout. ContextShift and Timer are provided by the environment and in this case the environment is the IOApp. Monix’s Task however has global ContextShift[Task] and Timer[Task] always in scope and doesn’t need them, but it does need a Scheduler to be available for the necessary Effect instance. And both Cats-Effect’s IO and Monix’s Task are cancelable, in which case it is desirable for the IOApp / TaskApp to install shutdown handlers to execute in case of interruption, however our type classes can also work with non-cancelable data types, in which case handling interruption is no longer necessary. Long story short, it’s better for IOApp to be specialized and each F[_] can come with its own app data type that is better suited for its needs. For example Monix’s Task comes with its own TaskApp. That said IOApp can be used for any F[_], because any Effect or ConcurrentEffect can be converted to IO. Example: import cats.effect._ import cats.data.EitherT object Main extends IOApp { type F[A] = EitherT[IO, Throwable, A] val F = implicitly[ConcurrentEffect[F]] def run(args: List[String]) = F.toIO { EitherT.right(IO(println(\"Hello from EitherT\"))) .map(_ =&gt; ExitCode.Success) } } Final Words IOApp is awesome for describing pure FP programs and gives you functionality that does not come for free when using the normal Java main protocol, like the interruption handler. And we worked hard to make this behavior available on top of JavaScript, via Scala.js, so you can use this for your Node.js apps as well."
    } ,    
    {
      "title": "LiftIO",
      "url": "/cats-effect/typeclasses/liftio.html",
      "content": "A Monad that can convert any given IO[A] into a F[A], useful for defining parametric signatures and composing monad transformer stacks. import cats.effect.IO trait LiftIO[F[_]] { def liftIO[A](ioa: IO[A]): F[A] } Let’s say your effect stack in your app is the following: import cats.effect.{LiftIO, IO} import scala.concurrent.Future type MyEffect[A] = Future[Either[Throwable, A]] implicit def myEffectLiftIO: LiftIO[MyEffect] = new LiftIO[MyEffect] { override def liftIO[A](ioa: IO[A]): MyEffect[A] = { ioa.attempt.unsafeToFuture() } } Having an implicit instance of LiftIO in scope you can, at any time, lift any given IO[A] into MyEffect[A]: val ioa: IO[String] = IO(\"Hello World!\") val effect: MyEffect[String] = LiftIO[MyEffect].liftIO(ioa) A more interesting example: import cats.data.EitherT import scala.concurrent.ExecutionContext.Implicits.global val L = implicitly[LiftIO[MyEffect]] val service1: MyEffect[Int] = Future.successful(Right(22)) val service2: MyEffect[Boolean] = Future.successful(Right(true)) val service3: MyEffect[String] = Future.successful(Left(new Exception(\"boom!\"))) val program: MyEffect[String] = (for { _ &lt;- EitherT(service1) x &lt;- EitherT(service2) y &lt;- EitherT(if (x) L.liftIO(IO(\"from io\")) else service3) } yield y).value"
    } ,      
    {
      "title": "MVar",
      "url": "/cats-effect/concurrency/mvar.html",
      "content": "An MVar2 is a mutable location that can be empty or contain a value, asynchronously blocking reads when empty and blocking writes when full. abstract class MVar2[F[_], A] { def put(a: A): F[Unit] def swap(b: A): F[A] def take: F[A] def read: F[A] def tryPut(a: A): F[Boolean] def tryTake: F[Option[A]] def tryRead: F[Option[A]] } Update 2.2.0: MVar2 adds new methods to the MVar interface without breaking binary compatibility. Please upgrade to MVar2 that supports tryRead and swap. Introduction Use-cases: As synchronized, thread-safe mutable variables As channels, with take and put acting as “receive” and “send” As a binary semaphore, with take and put acting as “acquire” and “release” It has these fundamental (atomic) operations: put: fills the MVar if it is empty, or blocks (asynchronously) if the MVar is full, until the given value is next in line to be consumed on take take: tries reading the current value (also emptying it), or blocks (asynchronously) until there is a value available, at which point the operation resorts to a take followed by a put read: which reads the current value without modifying the MVar, assuming there is a value available, or otherwise it waits until a value is made available via put swap: put a new value and return the taken one. It is not atomic. tryRead: returns the value immediately and None if it is empty. tryPut and tryTake variants of the above, that try those operation once and fail in case (semantic) blocking would be involved In this context “asynchronous blocking” means that we are not blocking any threads. Instead the implementation uses callbacks to notify clients when the operation has finished (notifications exposed by means of Async or Concurrent data types such as IO) and it thus works on top of JavaScript as well. Inspiration This data type is inspired by Control.Concurrent.MVar from Haskell, introduced in the paper Concurrent Haskell, by Simon Peyton Jones, Andrew Gordon and Sigbjorn Finne, though some details of their implementation are changed (in particular, a put on a full MVar used to error, but now merely blocks). Appropriate for building synchronization primitives and performing simple interthread communication, it’s the equivalent of a BlockingQueue(capacity = 1), except that there’s no actual thread blocking involved and it is powered by data types such as IO. Use-case: Synchronized Mutable Variables import cats.effect.concurrent._ def sum(state: MVar2[IO, Int], list: List[Int]): IO[Int] = list match { case Nil =&gt; state.take case x :: xs =&gt; state.take.flatMap { current =&gt; state.put(current + x).flatMap(_ =&gt; sum(state, xs)) } } MVar.of[IO, Int](0).flatMap(sum(_, (0 until 100).toList)) This sample isn’t very useful, except to show how MVar can be used as a variable. The take and put operations are atomic. The take call will (asynchronously) block if there isn’t a value available, whereas the call to put blocks if the MVar already has a value in it waiting to be consumed. Obviously after the call for take and before the call for put happens we could have concurrent logic that can update the same variable. While the two operations are atomic by themselves, a combination of them isn’t atomic (i.e. atomic operations don’t compose), therefore if we want this sample to be safe, then we need extra synchronization. Use-case: Asynchronous Lock (Binary Semaphore, Mutex) The take operation can act as “acquire” and put can act as the “release”. Let’s do it: final class MLock(mvar: MVar2[IO, Unit]) { def acquire: IO[Unit] = mvar.take def release: IO[Unit] = mvar.put(()) def greenLight[A](fa: IO[A]): IO[A] = acquire.bracket(_ =&gt; fa)(_ =&gt; release) } object MLock { def apply(): IO[MLock] = MVar[IO].of(()).map(ref =&gt; new MLock(ref)) } Use-case: Producer/Consumer Channel An obvious use-case is to model a simple producer-consumer channel. Say that you have a producer that needs to push events. But we also need some back-pressure, so we need to wait on the consumer to consume the last event before being able to generate a new event. import cats.effect._ import cats.effect.concurrent._ // Signaling option, because we need to detect completion type Channel[A] = MVar2[IO, Option[A]] def producer(ch: Channel[Int], list: List[Int]): IO[Unit] = list match { case Nil =&gt; ch.put(None) // we are done! case head :: tail =&gt; // next please ch.put(Some(head)).flatMap(_ =&gt; producer(ch, tail)) } def consumer(ch: Channel[Int], sum: Long): IO[Long] = ch.take.flatMap { case Some(x) =&gt; // next please consumer(ch, sum + x) case None =&gt; IO.pure(sum) // we are done! } // ContextShift required for // 1) MVar.empty // 2) IO.start // for ContextShift documentation see https://typelevel.org/cats-effect/datatypes/contextshift.html import scala.concurrent.ExecutionContext implicit val cs = IO.contextShift(ExecutionContext.Implicits.global) for { channel &lt;- MVar[IO].empty[Option[Int]] count = 100000 producerTask = producer(channel, (0 until count).toList) consumerTask = consumer(channel, 0L) fp &lt;- producerTask.start fc &lt;- consumerTask.start _ &lt;- fp.join sum &lt;- fc.join } yield sum Running this will work as expected. Our producer pushes values into our MVar and our consumer will consume all of those values."
    } ,    
    {
      "title": "Ref",
      "url": "/cats-effect/concurrency/ref.html",
      "content": "An asynchronous, concurrent mutable reference. abstract class Ref[F[_], A] { def get: F[A] def set(a: A): F[Unit] def modify[B](f: A =&gt; (A, B)): F[B] // ... and more } Provides safe concurrent access and modification of its content, but no functionality for synchronisation, which is instead handled by Deferred. For this reason, a Ref is always initialised to a value. The default implementation is nonblocking and lightweight, consisting essentially of a purely functional wrapper over an AtomicReference. Concurrent Counter This is probably one of the most common uses of this concurrency primitive. The workers will concurrently run and modify the value of the Ref so this is one possible outcome showing “#worker » currentCount”: #2 &gt;&gt; 0 #1 &gt;&gt; 0 #3 &gt;&gt; 0 #1 &gt;&gt; 0 #3 &gt;&gt; 2 #2 &gt;&gt; 1 import cats.effect.{IO, Sync} import cats.effect.concurrent.Ref import cats.syntax.all._ import scala.concurrent.ExecutionContext // Needed for triggering evaluation in parallel implicit val ctx = IO.contextShift(ExecutionContext.global) class Worker[F[_]](number: Int, ref: Ref[F, Int])(implicit F: Sync[F]) { private def putStrLn(value: String): F[Unit] = F.delay(println(value)) def start: F[Unit] = for { c1 &lt;- ref.get _ &lt;- putStrLn(show\"#$number &gt;&gt; $c1\") c2 &lt;- ref.modify(x =&gt; (x + 1, x)) _ &lt;- putStrLn(show\"#$number &gt;&gt; $c2\") } yield () } val program: IO[Unit] = for { ref &lt;- Ref.of[IO, Int](0) w1 = new Worker[IO](1, ref) w2 = new Worker[IO](2, ref) w3 = new Worker[IO](3, ref) _ &lt;- List( w1.start, w2.start, w3.start ).parSequence.void } yield ()"
    } ,    
    {
      "title": "Resource",
      "url": "/cats-effect/datatypes/resource.html",
      "content": "Effectfully allocates and releases a resource. Forms a MonadError on the resource type when the effect type has a Bracket instance. The Acquiring and releasing Resources section of the tutorial provides some additional context and examples regarding Resource. import cats.effect.BracketThrow abstract class Resource[F[_], A] { def use[B](f: A =&gt; F[B])(implicit F: BracketThrow[F]): F[B] } Nested resources are released in reverse order of acquisition. Outer resources are released even if an inner acquisition, use or release fails. You can lift any F[A] with an Applicative instance into a Resource[F, A] with a no-op release via Resource.liftF: import cats.effect.{IO, Resource} val greet: String =&gt; IO[Unit] = x =&gt; IO(println(\"Hello \" + x)) // greet: String =&gt; IO[Unit] = &lt;function1&gt; Resource.liftF(IO.pure(\"World\")).use(greet).unsafeRunSync() // Hello World Moreover it’s possible to apply further effects to the wrapped resource without leaving the Resource context via evalMap: import cats.effect.{IO, Resource} val acquire: IO[String] = IO(println(\"Acquire cats...\")) *&gt; IO(\"cats\") // acquire: IO[String] = Bind( // source = Delay(thunk = &lt;function0&gt;), // f = cats.effect.IO$$Lambda$11922/0x00000008030d7040@55f4401b, // trace = StackTrace( // stackTrace = List( // cats.effect.internals.IOTracing$.buildFrame(IOTracing.scala:48), // cats.effect.internals.IOTracing$.buildCachedFrame(IOTracing.scala:39), // cats.effect.internals.IOTracing$.cached(IOTracing.scala:34), // cats.effect.IO.flatMap(IO.scala:133), // cats.effect.IO.$times$greater(IO.scala:770), // repl.MdocSession$App7.&lt;init&gt;(io.md:203), // repl.MdocSession$App6.&lt;init&gt;(io.md:177), // repl.MdocSession$App5.&lt;init&gt;(io.md:155), // repl.MdocSession$App.&lt;init&gt;(io.md:137), // repl.MdocSession$.app(io.md:3), // mdoc.internal.document.DocumentBuilder$$doc$.$anonfun$build$2(DocumentBuilder.scala:89), // scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18), // scala.util.DynamicVariable.withValue(DynamicVariable.scala:59), // scala.Console$.withErr(Console.scala:193), // mdoc.internal.document.DocumentBuilder$$doc$.$anonfun$build$1(DocumentBuilder.scala:89), // scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18), // scala.util.DynamicVariable.withValue(DynamicVariable.scala:59), // scala.Console$.withOut(Console.scala:164), // mdoc.internal.document.DocumentBuilder$$doc$.build(DocumentBuilder.scala:88), // mdoc.internal.markdown.MarkdownBuilder$.buildDocument(MarkdownBuilder.scala:44), // mdoc.internal.markdown.Processor.processScalaInputs(Processor.scala:182), // mdoc.internal.markdown.Processor.processScalaInputs(Processor.scala:149), // mdoc.internal.markdown.Processor.processDocument(Processor.scala:49), // mdoc.internal.markdown.Markdown$.toMarkdown(Markdown.scala:131), // mdoc.internal.cli.MainOps.handleMarkdown(MainOps.scala:82), // mdoc.internal.cli.MainOps.handleFile(MainOps.scala:110), // mdoc.internal.cli.MainOps.$anonfun$generateCompleteSite$1(MainOps.scala:156), // scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:169), // scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:165), // scala.collection.immutable.List.foldLeft(List.scala:79), // mdoc.internal.cli.MainOps.generateCompleteSite(MainOps.scala:155), // mdoc.internal.cli.MainOps.run(MainOps.scala:177), // mdoc.internal.cli.MainOps$.process(MainOps.scala:269), // mdoc.Main$.process(Main.scala:26), // mdoc.Main$.process(Main.scala:21), // mdoc.Main$.main(Main.scala:16), // mdoc.Main.main(Main.scala), // java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), // ... val release: String =&gt; IO[Unit] = _ =&gt; IO(println(\"...release everything\")) // release: String =&gt; IO[Unit] = &lt;function1&gt; val addDogs: String =&gt; IO[String] = x =&gt; IO(println(\"...more animals...\")) *&gt; IO.pure(x + \" and dogs\") // addDogs: String =&gt; IO[String] = &lt;function1&gt; val report: String =&gt; IO[String] = x =&gt; IO(println(\"...produce weather report...\")) *&gt; IO(\"It's raining \" + x) // report: String =&gt; IO[String] = &lt;function1&gt; Resource.make(acquire)(release).evalMap(addDogs).use(report).unsafeRunSync() // Acquire cats... // ...more animals... // ...produce weather report... // ...release everything // res3: String = \"It's raining cats and dogs\" Example import cats.effect.{IO, Resource} def mkResource(s: String) = { val acquire = IO(println(s\"Acquiring $s\")) *&gt; IO.pure(s) def release(s: String) = IO(println(s\"Releasing $s\")) Resource.make(acquire)(release) } val r = for { outer &lt;- mkResource(\"outer\") inner &lt;- mkResource(\"inner\") } yield (outer, inner) r.use { case (a, b) =&gt; IO(println(s\"Using $a and $b\")) }.unsafeRunSync() If using an AutoCloseable create a resource without the need to specify how to close. Examples With scala.io.Source import cats.effect._ val acquire = IO { scala.io.Source.fromString(\"Hello world\") } Resource.fromAutoCloseable(acquire).use(source =&gt; IO(println(source.mkString))).unsafeRunSync() With java.io using IO import java.io._ import scala.jdk.CollectionConverters._ import cats.effect._ def readAllLines(bufferedReader: BufferedReader, blocker: Blocker)(implicit cs: ContextShift[IO]): IO[List[String]] = blocker.delay[IO, List[String]] { bufferedReader.lines().iterator().asScala.toList } def reader(file: File, blocker: Blocker)(implicit cs: ContextShift[IO]): Resource[IO, BufferedReader] = Resource.fromAutoCloseableBlocking(blocker)(IO { new BufferedReader(new FileReader(file)) } ) def readLinesFromFile(file: File, blocker: Blocker)(implicit cs: ContextShift[IO]): IO[List[String]] = { reader(file, blocker).use(br =&gt; readAllLines(br, blocker)) } A java.io example agnostic of the effect type import java.io._ import cats.effect._ import cats.syntax.flatMap._ def reader[F[_]](file: File, blocker: Blocker)(implicit F: Sync[F], cs: ContextShift[F]): Resource[F, BufferedReader] = Resource.fromAutoCloseableBlocking(blocker)(F.delay { new BufferedReader(new FileReader(file)) }) def dumpResource[F[_]](res: Resource[F, BufferedReader], blocker: Blocker)(implicit F: Sync[F], cs: ContextShift[F]): F[Unit] = { def loop(in: BufferedReader): F[Unit] = F.suspend { blocker.delay(in.readLine()).flatMap { line =&gt; if (line != null) { System.out.println(line) loop(in) } else { F.unit } } } res.use(loop) } def dumpFile[F[_]](file: File, blocker: Blocker)(implicit F: Sync[F], cs: ContextShift[F]): F[Unit] = dumpResource(reader(file, blocker), blocker)"
    } ,      
    {
      "title": "Semaphore",
      "url": "/cats-effect/concurrency/semaphore.html",
      "content": "A semaphore has a non-negative number of permits available. Acquiring a permit decrements the current number of permits and releasing a permit increases the current number of permits. An acquire that occurs when there are no permits available results in semantic blocking until a permit becomes available. abstract class Semaphore[F[_]] { def available: F[Long] def acquire: F[Unit] def release: F[Unit] // ... and more } Semantic Blocking and Cancellation Semaphore does what we call “semantic” blocking, meaning that no actual threads are being blocked while waiting to acquire a permit. Behavior on cancellation: Blocking acquires are cancelable if the semaphore is created with Semaphore.apply (and hence, with a Concurrent[F] instance). Blocking acquires are non-cancelable if the semaphore is created with Semaphore.uncancelable (and hence, with an Async[F] instance). Shared Resource When multiple processes try to access a precious resource you might want to constraint the number of accesses. Here is where Semaphore[F] is useful. Three processes are trying to access a shared resource at the same time but only one at a time will be granted access and the next process have to wait until the resource gets available again (availability is one as indicated by the semaphore counter). R1, R2 &amp; R3 will request access of the precious resource concurrently so this could be one possible outcome: R1 &gt;&gt; Availability: 1 R2 &gt;&gt; Availability: 1 R2 &gt;&gt; Started | Availability: 0 R3 &gt;&gt; Availability: 0 -------------------------------- R1 &gt;&gt; Started | Availability: 0 R2 &gt;&gt; Done | Availability: 0 -------------------------------- R3 &gt;&gt; Started | Availability: 0 R1 &gt;&gt; Done | Availability: 0 -------------------------------- R3 &gt;&gt; Done | Availability: 1 This means when R1 and R2 requested the availability it was one and R2 was faster in getting access to the resource so it started processing. R3 was the slowest and saw that there was no availability from the beginning. Once R2 was done R1 started processing immediately showing no availability. Once R1 was done R3 started processing immediately showing no availability. Finally, R3 was done showing an availability of one once again. import cats.effect.{Concurrent, IO, Timer} import cats.effect.concurrent.Semaphore import cats.syntax.all._ import scala.concurrent.ExecutionContext import scala.concurrent.duration._ // Needed for getting a Concurrent[IO] instance implicit val ctx = IO.contextShift(ExecutionContext.global) // Needed for `sleep` implicit val timer = IO.timer(ExecutionContext.global) class PreciousResource[F[_]](name: String, s: Semaphore[F])(implicit F: Concurrent[F], timer: Timer[F]) { def use: F[Unit] = for { x &lt;- s.available _ &lt;- F.delay(println(s\"$name &gt;&gt; Availability: $x\")) _ &lt;- s.acquire y &lt;- s.available _ &lt;- F.delay(println(s\"$name &gt;&gt; Started | Availability: $y\")) _ &lt;- timer.sleep(3.seconds) _ &lt;- s.release z &lt;- s.available _ &lt;- F.delay(println(s\"$name &gt;&gt; Done | Availability: $z\")) } yield () } val program: IO[Unit] = for { s &lt;- Semaphore[IO](1) r1 = new PreciousResource[IO](\"R1\", s) r2 = new PreciousResource[IO](\"R2\", s) r3 = new PreciousResource[IO](\"R3\", s) _ &lt;- List(r1.use, r2.use, r3.use).parSequence.void } yield ()"
    } ,    
    {
      "title": "Sync",
      "url": "/cats-effect/typeclasses/sync.html",
      "content": "A Monad that can suspend the execution of side effects in the F[_] context. import cats.Defer import cats.effect.BracketThrow trait Sync[F[_]] extends BracketThrow[F] with Defer[F] { def suspend[A](thunk: =&gt; F[A]): F[A] def delay[A](thunk: =&gt; A): F[A] = suspend(pure(thunk)) } This is the most basic interface that represents the suspension of synchronous side effects. On the other hand, its implementation of flatMap is stack safe, meaning that you can describe tailRecM in terms of it as demonstrated in the laws module. import cats.effect.{IO, Sync} import cats.laws._ val F = Sync[IO] lazy val stackSafetyOnRepeatedRightBinds = { val result = (0 until 10000).foldRight(F.delay(())) { (_, acc) =&gt; F.delay(()).flatMap(_ =&gt; acc) } result &lt;-&gt; F.pure(()) } Example of use: import cats.effect.{IO, Sync} val ioa = Sync[IO].delay(println(\"Hello world!\")) ioa.unsafeRunSync() So basically using Sync[IO].delay is equivalent to using IO.apply. The use of suspend is useful for trampolining (i.e. when the side effect is conceptually the allocation of a stack frame) and it’s used by delay to represent an internal stack of calls. Any exceptions thrown by the side effect will be caught and sequenced into the F[_] context."
    } ,    
    {
      "title": "SyncIO",
      "url": "/cats-effect/datatypes/syncio.html",
      "content": "A pure abstraction representing the intention to perform a side effect, where the result of that side effect is obtained synchronously. SyncIO is similar to IO, but does not support asynchronous computations. Consequently, a SyncIO can be run synchronously to obtain a result via unsafeRunSync. This is unlike IO#unsafeRunSync, which cannot be safely called in general. Doing so on the JVM blocks the calling thread while the async part of the computation is run and doing so on Scala.js throws an exception upon encountering an async boundary. Constructing SyncIO values import cats.effect.SyncIO def putStrLn(str: String): SyncIO[Unit] = SyncIO(println(str)) SyncIO.pure(\"Cats!\").flatMap(putStrLn).unsafeRunSync() // Cats! There’s also suspend and unit, equivalent to the same operations defined in IO but with synchronicity guarantees. Interoperation with Eval and IO SyncIO defines an eval method in its companion object to lift any cats.Eval value. import cats.Eval val eval = Eval.now(\"hey!\") // eval: Eval[String] = Now(value = \"hey!\") SyncIO.eval(eval).unsafeRunSync() // res1: String = \"hey!\" SyncIO also defines a to[F] method at the class level to lift your value into any F with a LiftIO instance available. import cats.effect.IO val ioa: SyncIO[Unit] = SyncIO(println(\"Hello world!\")) // ioa: SyncIO[Unit] = SyncIO$1965096469 val iob: IO[Unit] = ioa.to[IO] // iob: IO[Unit] = Delay(thunk = &lt;function0&gt;) iob.unsafeRunAsync(_ =&gt; ()) // Hello world!"
    } ,    
    {
      "title": "Timer",
      "url": "/cats-effect/datatypes/timer.html",
      "content": "It is a scheduler of tasks. You can think of it as the purely functional equivalent of: Java’s ScheduledExecutorService. JavaScript’s setTimeout. It provides: The ability to get the current time. Ability to delay the execution of a task with a specified time duration. It does all of that in an F[_] monadic context that can suspend side effects and is capable of asynchronous execution (e.g. IO). This is NOT a typeclass, as it does not have the coherence requirement. import cats.effect.Clock import scala.concurrent.duration.FiniteDuration trait Timer[F[_]] { def clock: Clock[F] def sleep(duration: FiniteDuration): F[Unit] } As mentioned in the IO documentation, there’s a default instance of Timer[IO] available. However, you might want to implement your own to have a fine-grained control over your thread pools. You can look at the mentioned implementation for more details, but it roughly looks like this: import java.util.concurrent.ScheduledExecutorService import cats.effect.{IO, Timer, Clock} import scala.concurrent.ExecutionContext import scala.concurrent.duration._ final class MyTimer(ec: ExecutionContext, sc: ScheduledExecutorService) extends Timer[IO] { override val clock: Clock[IO] = new Clock[IO] { override def realTime(unit: TimeUnit): IO[Long] = IO(unit.convert(System.currentTimeMillis(), MILLISECONDS)) override def monotonic(unit: TimeUnit): IO[Long] = IO(unit.convert(System.nanoTime(), NANOSECONDS)) } override def sleep(timespan: FiniteDuration): IO[Unit] = IO.cancelable { cb =&gt; val tick = new Runnable { def run() = ec.execute(new Runnable { def run() = cb(Right(())) }) } val f = sc.schedule(tick, timespan.length, timespan.unit) IO(f.cancel(false)).void } } Configuring the global Scheduler The one-argument overload of IO.timer lazily instantiates a global ScheduledExecutorService, which is never shut down. This is fine for most applications, but leaks threads when the class is repeatedly loaded in the same JVM, as is common in testing. The global scheduler can be configured with the following system properties: cats.effect.global_scheduler.threads.core_pool_size: sets the core pool size of the global scheduler. Defaults to 2. cats.effect.global_scheduler.keep_alive_time_ms: allows the global scheduler’s core threads to timeout and terminate when idle. 0 keeps the threads from timing out. Defaults to 0. Value is in milliseconds. These properties only apply on the JVM. Also see these related data types: Clock: for time measurements and getting the current clock ContextShift: the pure equivalent of an ExecutionContext"
    } ,    
    {
      "title": "Tutorial",
      "url": "/cats-effect/tutorial/tutorial.html",
      "content": "Introduction This tutorial tries to help newcomers to cats-effect to get familiar with its main concepts by means of code examples, in a learn-by-doing fashion. Two small programs will be coded, each one in its own section. The first one copies the contents from one file to another, safely handling resources and cancellation in the process. That should help us to flex our muscles. The second one implements a solution to the producer-consumer problem to introduce cats-effect fibers. This tutorial assumes certain familiarity with functional programming. It is also a good idea to read cats-effect documentation prior to starting this tutorial, at least the excellent documentation about IO data type. Please read this tutorial as training material, not as a best-practices document. As you gain more experience with cats-effect, probably you will find your own solutions to deal with the problems presented here. Also, bear in mind that using cats-effect for copying files or implementing basic concurrency patterns (such as the producer-consumer problem) is suitable for a ‘getting things done’ approach, but for more complex systems/settings/requirements you might want to take a look at fs2 or Monix to find powerful network and file abstractions that integrate with cats-effect. But that is beyond the purpose of this tutorial, which focuses solely on cats-effect. That said, let’s go! Setting things up This Github repo includes all the software that will be developed during this tutorial. It uses sbt as the build tool. To ease coding, compiling and running the code snippets in this tutorial it is recommended to use the same build.sbt, or at least one with the same dependencies and compilation options: name := \"cats-effect-tutorial\" version := \"2.2.0\" scalaVersion := \"2.12.8\" libraryDependencies += \"org.typelevel\" %% \"cats-effect\" % \"2.2.0\" withSources() withJavadoc() scalacOptions ++= Seq( \"-feature\", \"-deprecation\", \"-unchecked\", \"-language:postfixOps\", \"-language:higherKinds\", \"-Ypartial-unification\") Code snippets in this tutorial can be pasted and compiled right in the scala console of the project defined above (or any project with similar settings). Copying files - basic concepts, resource handling and cancellation Our goal is to create a program that copies files. First we will work on a function that carries such task, and then we will create a program that can be invoked from the shell and uses that function. First of all we must code the function that copies the content from a file to another file. The function takes the source and destination files as parameters. But this is functional programming! So invoking the function shall not copy anything, instead it will return an IO instance that encapsulates all the side effects involved (opening/closing files, reading/writing content), that way purity is kept. Only when that IO instance is evaluated all those side-effectful actions will be run. In our implementation the IO instance will return the amount of bytes copied upon execution, but this is just a design decision. Of course errors can occur, but when working with any IO those should be embedded in the IO instance. That is, no exception is raised outside the IO and so no try (or the like) needs to be used when using the function, instead the IO evaluation will fail and the IO instance will carry the error raised. Now, the signature of our function looks like this: import cats.effect.IO import java.io.File def copy(origin: File, destination: File): IO[Long] = ??? Nothing scary, uh? As we said before, the function just returns an IO instance. When run, all side-effects will be actually executed and the IO instance will return the bytes copied in a Long (note that IO is parameterized by the return type). Now, let’s start implementing our function. First, we need to open two streams that will read and write file contents. Acquiring and releasing Resources We consider opening a stream to be a side-effect action, so we have to encapsulate those actions in their own IO instances. For this, we will make use of cats-effect Resource, that allows to orderly create, use and then release resources. See this code: import cats.effect.{IO, Resource} import cats.syntax.all._ import java.io._ def inputStream(f: File): Resource[IO, FileInputStream] = Resource.make { IO(new FileInputStream(f)) // build } { inStream =&gt; IO(inStream.close()).handleErrorWith(_ =&gt; IO.unit) // release } def outputStream(f: File): Resource[IO, FileOutputStream] = Resource.make { IO(new FileOutputStream(f)) // build } { outStream =&gt; IO(outStream.close()).handleErrorWith(_ =&gt; IO.unit) // release } def inputOutputStreams(in: File, out: File): Resource[IO, (InputStream, OutputStream)] = for { inStream &lt;- inputStream(in) outStream &lt;- outputStream(out) } yield (inStream, outStream) We want to ensure that streams are closed once we are done using them, no matter what. That is precisely why we use Resource in both inputStream and outputStream functions, each one returning one Resource that encapsulates the actions for opening and then closing each stream. inputOutputStreams encapsulates both resources in a single Resource instance that will be available once the creation of both streams has been successful, and only in that case. As seen in the code above Resource instances can be combined in for-comprehensions as they implement flatMap. Note also that when releasing resources we must also take care of any possible error during the release itself, for example with the .handleErrorWith call as we do in the code above. In this case we just swallow the error, but normally it should be at least logged. Optionally we could have used Resource.fromAutoCloseable to define our resources, that method creates Resource instances over objects that implement java.lang.AutoCloseable interface without having to define how the resource is released. So our inputStream function would look like this: import cats.effect.{IO, Resource} import java.io.{File, FileInputStream} def inputStream(f: File): Resource[IO, FileInputStream] = Resource.fromAutoCloseable(IO(new FileInputStream(f))) That code is way simpler, but with that code we would not have control over what would happen if the closing operation throws an exception. Also it could be that we want to be aware when closing operations are being run, for example using logs. In contrast, using Resource.make allows to easily control the actions of the release phase. Let’s go back to our copy function, which now looks like this: import cats.effect.{IO, Resource} import java.io._ // as defined before def inputOutputStreams(in: File, out: File): Resource[IO, (InputStream, OutputStream)] = ??? // transfer will do the real work def transfer(origin: InputStream, destination: OutputStream): IO[Long] = ??? def copy(origin: File, destination: File): IO[Long] = inputOutputStreams(origin, destination).use { case (in, out) =&gt; transfer(in, out) } The new method transfer will perform the actual copying of data, once the resources (the streams) are obtained. When they are not needed anymore, whatever the outcome of transfer (success or failure) both streams will be closed. If any of the streams could not be obtained, then transfer will not be run. Even better, because of Resource semantics, if there is any problem opening the input file then the output file will not be opened. On the other hand, if there is any issue opening the output file, then the input stream will be closed. What about bracket? Now, if you are familiar with cats-effect’s Bracket you may be wondering why we are not using it as it looks so similar to Resource (and there is a good reason for that: Resource is based on bracket). Ok, before moving forward it is worth to take a look to bracket. There are three stages when using bracket: resource acquisition, usage, and release. Each stage is defined by an IO instance. A fundamental property is that the release stage will always be run regardless whether the usage stage finished correctly or an exception was thrown during its execution. In our case, in the acquisition stage we would create the streams, then in the usage stage we will copy the contents, and finally in the release stage we will close the streams. Thus we could define our copy function as follows: import cats.effect.IO import cats.syntax.all._ import java.io._ // function inputOutputStreams not needed // transfer will do the real work def transfer(origin: InputStream, destination: OutputStream): IO[Long] = ??? def copy(origin: File, destination: File): IO[Long] = { val inIO: IO[InputStream] = IO(new FileInputStream(origin)) val outIO:IO[OutputStream] = IO(new FileOutputStream(destination)) (inIO, outIO) // Stage 1: Getting resources .tupled // From (IO[InputStream], IO[OutputStream]) to IO[(InputStream, OutputStream)] .bracket{ case (in, out) =&gt; transfer(in, out) // Stage 2: Using resources (for copying data, in this case) } { case (in, out) =&gt; // Stage 3: Freeing resources (IO(in.close()), IO(out.close())) .tupled // From (IO[Unit], IO[Unit]) to IO[(Unit, Unit)] .handleErrorWith(_ =&gt; IO.unit).void } } New copy definition is more complex, even though the code as a whole is way shorter as we do not need the inputOutputStreams function. But there is a catch in the code above. When using bracket, if there is a problem when getting resources in the first stage, then the release stage will not be run. Now, in the code above, first the origin file and then the destination file are opened (tupled just reorganizes both IO instances into a single one). So what would happen if we successfully open the origin file (i.e. when evaluating inIO) but then an exception is raised when opening the destination file (i.e. when evaluating outIO)? In that case the origin stream will not be closed! To solve this we should first get the first stream with one bracket call, and then the second stream with another bracket call inside the first. But, in a way, that’s precisely what we do when we flatMap instances of Resource. And the code looks cleaner too. So, while using bracket directly has its place, Resource is likely to be a better choice when dealing with multiple resources at once. Copying data Finally we have our streams ready to go! We have to focus now on coding transfer. That function will have to define a loop that at each iteration reads data from the input stream into a buffer, and then writes the buffer contents into the output stream. At the same time, the loop will keep a counter of the bytes transferred. To reuse the same buffer we should define it outside the main loop, and leave the actual transmission of data to another function transmit that uses that loop. Something like: import cats.effect.IO import cats.syntax.all._ import java.io._ def transmit(origin: InputStream, destination: OutputStream, buffer: Array[Byte], acc: Long): IO[Long] = for { amount &lt;- IO(origin.read(buffer, 0, buffer.size)) count &lt;- if(amount &gt; -1) IO(destination.write(buffer, 0, amount)) &gt;&gt; transmit(origin, destination, buffer, acc + amount) else IO.pure(acc) // End of read stream reached (by java.io.InputStream contract), nothing to write } yield count // Returns the actual amount of bytes transmitted def transfer(origin: InputStream, destination: OutputStream): IO[Long] = for { buffer &lt;- IO(new Array[Byte](1024 * 10)) // Allocated only when the IO is evaluated total &lt;- transmit(origin, destination, buffer, 0L) } yield total Take a look at transmit, observe that both input and output actions are encapsulated in (suspended in) IO. IO being a monad, we can sequence them using a for-comprehension to create another IO. The for-comprehension loops as long as the call to read() does not return a negative value that would signal that the end of the stream has been reached. &gt;&gt; is a Cats operator to sequence two operations where the output of the first is not needed by the second (i.e. it is equivalent to first.flatMap(_ =&gt; second)). In the code above that means that after each write operation we recursively call transmit again, but as IO is stack safe we are not concerned about stack overflow issues. At each iteration we increase the counter acc with the amount of bytes read at that iteration. We are making progress, and already have a version of copy that can be used. If any exception is raised when transfer is running, then the streams will be automatically closed by Resource. But there is something else we have to take into account: IO instances execution can be canceled!. And cancellation should not be ignored, as it is a key feature of cats-effect. We will discuss cancellation in the next section. Dealing with cancellation Cancellation is a powerful but non-trivial cats-effect feature. In cats-effect, some IO instances can be canceled ( e.g. by other IO instaces running concurrently) meaning that their evaluation will be aborted. If the programmer is careful, an alternative IO task will be run under cancellation, for example to deal with potential cleaning up activities. Now, IOs created with Resource.use can be canceled. The cancellation will trigger the execution of the code that handles the closing of the resource. In our case, that would close both streams. So far so good! But what happens if cancellation happens while the streams are being used? This could lead to data corruption as a stream where some thread is writing to is at the same time being closed by another thread. For more info about this problem see Gotcha: Cancellation is a concurrent action in cats-effect site. To prevent such data corruption we must use some concurrency control mechanism that ensures that no stream will be closed while the IO returned by transfer is being evaluated. Cats-effect provides several constructs for controlling concurrency, for this case we will use a semaphore. A semaphore has a number of permits, its method .acquire ‘blocks’ if no permit is available until release is called on the same semaphore. It is important to remark that there is no actual thread being really blocked, the thread that finds the .acquire call will be immediately recycled by cats-effect. When the release method is invoked then cats-effect will look for some available thread to resume the execution of the code after .acquire. We will use a semaphore with a single permit. The .withPermit method acquires one permit, runs the IO given and then releases the permit. We could also use .acquire and then .release on the semaphore explicitly, but .withPermit is more idiomatic and ensures that the permit is released even if the effect run fails. import cats.syntax.all._ import cats.effect.{Concurrent, IO, Resource} import cats.effect.concurrent.Semaphore import java.io._ // transfer and transmit methods as defined before def transfer(origin: InputStream, destination: OutputStream): IO[Long] = ??? def inputStream(f: File, guard: Semaphore[IO]): Resource[IO, FileInputStream] = Resource.make { IO(new FileInputStream(f)) } { inStream =&gt; guard.withPermit { IO(inStream.close()).handleErrorWith(_ =&gt; IO.unit) } } def outputStream(f: File, guard: Semaphore[IO]): Resource[IO, FileOutputStream] = Resource.make { IO(new FileOutputStream(f)) } { outStream =&gt; guard.withPermit { IO(outStream.close()).handleErrorWith(_ =&gt; IO.unit) } } def inputOutputStreams(in: File, out: File, guard: Semaphore[IO]): Resource[IO, (InputStream, OutputStream)] = for { inStream &lt;- inputStream(in, guard) outStream &lt;- outputStream(out, guard) } yield (inStream, outStream) def copy(origin: File, destination: File)(implicit concurrent: Concurrent[IO]): IO[Long] = { for { guard &lt;- Semaphore[IO](1) count &lt;- inputOutputStreams(origin, destination, guard).use { case (in, out) =&gt; guard.withPermit(transfer(in, out)) } } yield count } Before calling to transfer we acquire the semaphore, which is not released until transfer is done. The use call ensures that the semaphore will be released under any circumstances, whatever the result of transfer (success, error, or cancellation). As the ‘release’ parts in the Resource instances are now blocked on the same semaphore, we can be sure that streams are closed only after transfer is over, i.e. we have implemented mutual exclusion of transfer execution and resources releasing. An implicit Concurrent instance is required to create the semaphore instance, which is included in the function signature. Mark that while the IO returned by copy is cancelable (because so are IO instances returned by Resource.use), the IO returned by transfer is not. Trying to cancel it will not have any effect and that IO will run until the whole file is copied! In real world code you will probably want to make your functions cancelable, section Building cancelable IO tasks of IO documentation explains how to create such cancelable IO instances (besides calling Resource.use, as we have done for our code). And that is it! We are done, now we can create a program that uses this copy function. IOApp for our final program We will create a program that copies files, this program only takes two parameters: the name of the origin and destination files. For coding this program we will use IOApp as it allows to maintain purity in our definitions up to the program main function. IOApp is a kind of ‘functional’ equivalent to Scala’s App, where instead of coding an effectful main method we code a pure run function. When executing the class a main method defined in IOApp will call the run function we have coded. Any interruption (like pressing Ctrl-c) will be treated as a cancellation of the running IO. Also IOApp provides implicit instances of Timer[IO] and ContextShift[IO] (not discussed yet in this tutorial). ContextShift[IO] allows for having a Concurrent[IO] in scope, as the one required by the copy function. When coding IOApp, instead of a main function we have a run function, which creates the IO instance that forms the program. In our case, our run method can look like this: import cats.effect._ import cats.syntax.all._ import java.io.File object Main extends IOApp { // copy as defined before def copy(origin: File, destination: File): IO[Long] = ??? override def run(args: List[String]): IO[ExitCode] = for { _ &lt;- if(args.length &lt; 2) IO.raiseError(new IllegalArgumentException(\"Need origin and destination files\")) else IO.unit orig = new File(args(0)) dest = new File(args(1)) count &lt;- copy(orig, dest) _ &lt;- IO(println(s\"$count bytes copied from ${orig.getPath} to ${dest.getPath}\")) } yield ExitCode.Success } Heed how run verifies the args list passed. If there are fewer than two arguments, an error is raised. As IO implements MonadError we can at any moment call to IO.raiseError to interrupt a sequence of IO operations. Copy program code You can check the final version of our copy program here. The program can be run from sbt just by issuing this call: &gt; runMain catseffecttutorial.CopyFile origin.txt destination.txt It can be argued that using IO{java.nio.file.Files.copy(...)} would get an IO with the same characteristics of purity as our function. But there is a difference: our IO is safely cancelable! So the user can stop the running code at any time for example by pressing Ctrl-c, our code will deal with safe resource release (streams closing) even under such circumstances. The same will apply if the copy function is run from other modules that require its functionality. If the IO returned by this function is canceled while being run, still resources will be properly released. But recall what we commented before: this is because use returns IO instances that are cancelable, in contrast our transfer function is not cancelable. WARNING: To properly test cancelation, You should also ensure that fork := true is set in the sbt configuration, otherwise sbt will intercept the cancelation because it will be running the program in the same JVM as itself. Polymorphic cats-effect code There is an important characteristic of IO that we shall be aware of. IO is able to encapsulate side-effects, but the capacity to define concurrent and/or async and/or cancelable IO instances comes from the existence of a Concurrent[IO] instance. Concurrent is a type class that, for an F[_] carrying a side-effect, brings the ability to cancel or start concurrently the side-effect in F. Concurrent also extends type class Async, that allows to define synchronous/asynchronous computations. Async, in turn, extends type class Sync, which can suspend the execution of side effects in F. So well, Sync can suspend side effects (and so can Async and Concurrent as they extend Sync). We have used IO so far mostly for that purpose. Now, going back to the code we created to copy files, could have we coded its functions in terms of some F[_]: Sync instead of IO? Truth is we could and in fact it is recommendable in real world programs. See for example how we would define a polymorphic version of our transfer function with this approach, just by replacing any use of IO by calls to the delay and pure methods of the Sync[F[_]] instance! import cats.effect.Sync import cats.syntax.all._ import java.io._ def transmit[F[_]: Sync](origin: InputStream, destination: OutputStream, buffer: Array[Byte], acc: Long): F[Long] = for { amount &lt;- Sync[F].delay(origin.read(buffer, 0, buffer.size)) count &lt;- if(amount &gt; -1) Sync[F].delay(destination.write(buffer, 0, amount)) &gt;&gt; transmit(origin, destination, buffer, acc + amount) else Sync[F].pure(acc) // End of read stream reached (by java.io.InputStream contract), nothing to write } yield count // Returns the actual amount of bytes transmitted We leave as an exercise to code the polymorphic versions of inputStream, outputStream, inputOutputStreams and transfer functions. You will find that transformation similar to the one shown for transfer in the snippet above. Function copy is different however. If you try to implement that function as well you will realize that we need a full instance of Concurrent[F] in scope, this is because it is required by the Semaphore instantiation: import cats.effect._ import cats.effect.concurrent.Semaphore import cats.syntax.all._ import java.io._ def transmit[F[_]: Sync](origin: InputStream, destination: OutputStream, buffer: Array[Byte], acc: Long): F[Long] = ??? def transfer[F[_]: Sync](origin: InputStream, destination: OutputStream): F[Long] = ??? def inputStream[F[_]: Sync](f: File, guard: Semaphore[F]): Resource[F, FileInputStream] = ??? def outputStream[F[_]: Sync](f: File, guard: Semaphore[F]): Resource[F, FileOutputStream] = ??? def inputOutputStreams[F[_]: Sync](in: File, out: File, guard: Semaphore[F]): Resource[F, (InputStream, OutputStream)] = ??? def copy[F[_]: Concurrent](origin: File, destination: File): F[Long] = for { guard &lt;- Semaphore[F](1) count &lt;- inputOutputStreams(origin, destination, guard).use { case (in, out) =&gt; guard.withPermit(transfer(in, out)) } } yield count Only in our main function we will set IO as the final F for our program. To do so, of course, a Concurrent[IO] instance must be in scope, but that instance is brought transparently by IOApp so we do not need to be concerned about it. During the remaining of this tutorial we will use polymorphic code, only falling to IO in the run method of our IOApps. Polymorphic code is less restrictive, as functions are not tied to IO but are applicable to any F[_] as long as there is an instance of the type class required (Sync[F[_]] , Concurrent[F[_]]…) in scope. The type class to use will depend on the requirements of our code. For example, if the execution of the side-effect should be cancelable, then we must stick to Concurrent[F[_]]. Also, it is actually easier to work on F than on any specific type. Copy program code, polymorphic version The polymorphic version of our copy program in full is available here. Exercises: improving our small IO program To finalize we propose you some exercises that will help you to keep improving your IO-kungfu: Modify the IOApp so it shows an error and abort the execution if the origin and destination files are the same, the origin file cannot be open for reading or the destination file cannot be opened for writing. Also, if the destination file already exists, the program should ask for confirmation before overwriting that file. Modify transmit so the buffer size is not hardcoded but passed as parameter. Use some other concurrency tool of cats-effect instead of semaphore to ensure mutual exclusion of transfer execution and streams closing. Create a new program able to copy folders. If the origin folder has subfolders, then their contents must be recursively copied too. Of course the copying must be safely cancelable at any moment. Producer-consumer problem - concurrency and fibers The producer-consumer pattern is often found in concurrent setups. Here one or more producers insert data on a shared data structure like a queue or buffer while one or more consumers extract data from it. Readers and writers run concurrently. If the queue is empty then readers will block until data is available, if the queue is full then writers will wait for some ‘bucket’ to be free. Only one writer at a time can add data to the queue to prevent data corruption. Also only one reader can extract data from the queue so no two readers get the same data item. Variations of this problem exists depending on whether there are more than one consumer/producer, or whether the data structure siting between them is size-bounded or not. Unless stated otherwise, the solutions discussed here are suited for multi consumer and multi reader settings. Initially the solutions will assume an unbounded data structure, to then present a solution for a bounded one. But before we work on the solution for this problem we must introduce fibers, which are the basic building block of cats-effect concurrency. Intro to fibers A fiber carries an F action to execute (typically an IO instance). Fibers are like ‘light’ threads, meaning they can be used in a similar way than threads to create concurrent code. However, they are not threads. Spawning new fibers does not guarantee that the action described in the F associated to it will be run if there is a shortage of threads. Internally cats-effect uses thread pools to run fibers. So if there is no thread available in the pool then the fiber execution will ‘wait’ until some thread is free again. On the other hand fibers are, unlike threads, very cheap entities. We can spawn millions of them at ease without impacting the performance. ContextShift[F] is in charge of assigning threads to the fibers waiting to be run. When using IOApp we get also the ContextShift[IO] that we need to run the fibers in our code. But the developer can also create new ContextShift[F] instances using custom thread pools. Cats-effect implements some concurrency primitives to coordinate concurrent fibers: Deferred, MVar2, Ref and Semaphore (semaphores we already discussed in the first part of this tutorial). It is important to understand that, when a fiber gets blocked by some concurrent data structure, cats-effect recycles the thread so it becomes available for other fibers. Cats-effect also recovers threads of finished and cancelled fibers. But keep in mind that, in contrast, if the fiber is blocked by some external action like waiting for some input from a TCP socket, then cats-effect has no way to recover back that thread until the action finishes. Way more detailed info about concurrency in cats-effect can be found in this other tutorial ‘Concurrency in Scala with Cats-Effect’. It is also strongly advised to read the Concurrency section of cats-effect docs. But for the remaining of this tutorial we will focus on a practical approach to those concepts. Ok, now we have briefly discussed fibers we can start working on our producer-consumer problem. First (and inefficient) implementation We need an intermediate structure where producer(s) can insert data to and consumer(s) extracts data from. Let’s assume a simple queue. Initially there will be only one producer and one consumer. Producer will generate a sequence of integers (1, 2, 3…), consumer will just read that sequence. Our shared queue will be an instance of an immutable Queue[Int]. As accesses to the queue can (and will!) be concurrent, we need some way to protect the queue so only one fiber at a time is handling it. The best way to ensure an ordered access to some shared data is Ref. A Ref instance wraps some given data and implements methods to manipulate that data in a safe manner. When some fiber is runnning one of those methods, any other call to any method of the Ref instance will be blocked. The Ref wrapping our queue will be Ref[F, Queue[Int]] (for some F[_]). Now, our producer method will be: import cats.effect._ import cats.effect.concurrent.Ref import cats.syntax.all._ import collection.immutable.Queue def producer[F[_]: Sync: ContextShift](queueR: Ref[F, Queue[Int]], counter: Int): F[Unit] = (for { _ &lt;- if(counter % 10000 == 0) Sync[F].delay(println(s\"Produced $counter items\")) else Sync[F].unit _ &lt;- queueR.getAndUpdate(_.enqueue(counter + 1)) // Putting data in shared queue _ &lt;- ContextShift[F].shift } yield ()) &gt;&gt; producer(queueR, counter + 1) First line just prints some log message every 10000 items, so we know if it is ‘alive’. Then it calls queueR.getAndUpdate to add data into the queue. Note that .getAndUpdate provides the current queue, then we use .enqueue to insert the next value counter+1. This call returns a new queue with the value added that is stored by the ref instance. If some other fiber is accessing to queueR then the fiber is blocked. Finally we run ContextShift[F].shift before calling again to the producer recursively with the next counter value. In fact, the call to .shift is not strictly needed but it is a good policy to include it in recursive functions. Why is that? Well, internally cats-effect tries to assign fibers so all tasks are given the chance to be executed. But in a recursive function that potentially never ends it can be that the fiber is always running and cats-effect has no change to recover the thread being used by it. By calling to .shift the programmer explicitly tells cats-effect that the current thread can be re-assigned to other fiber if so decides. Strictly speaking, maybe our producer does not need such call as the access to queueR.getAndUpdate will get the fiber blocked if some other fiber is using queueR at that moment, and cats-effect can recycle blocked fibers for other tasks. Still, we keep it there as good practice. The consumer method is a bit different. It will try to read data from the queue but it must be aware that the queue must be empty: import cats.effect._ import cats.effect.concurrent.Ref import cats.syntax.all._ import collection.immutable.Queue def consumer[F[_] : Sync: ContextShift](queueR: Ref[F, Queue[Int]]): F[Unit] = (for { iO &lt;- queueR.modify{ queue =&gt; queue.dequeueOption.fold((queue, Option.empty[Int])){case (i,queue) =&gt; (queue, Option(i))} } _ &lt;- if(iO.exists(_ % 10000 == 0)) Sync[F].delay(println(s\"Consumed ${iO.get} items\")) else Sync[F].unit _ &lt;- ContextShift[F].shift } yield iO) &gt;&gt; consumer(queueR) The call to queueR.modify allows to modify the wrapped data (our queue) and return a value that is computed from that data. In our case, it returns an Option[Int] that will be None if queue was empty. Next line is used to log a message in console every 10000 read items. Finally consumer is called recursively to start again. We can now create a program that instantiates our queueR and runs both producer and consumer in parallel: import cats.effect._ import cats.effect.concurrent.Ref import cats.syntax.all._ import collection.immutable.Queue object InefficientProducerConsumer extends IOApp { def producer[F[_]: Sync: ContextShift](queueR: Ref[F, Queue[Int]], counter: Int): F[Unit] = ??? // As defined before def consumer[F[_] : Sync: ContextShift](queueR: Ref[F, Queue[Int]]): F[Unit] = ??? // As defined before override def run(args: List[String]): IO[ExitCode] = for { queueR &lt;- Ref.of[IO, Queue[Int]](Queue.empty[Int]) res &lt;- (consumer(queueR), producer(queueR, 0)) .parMapN((_, _) =&gt; ExitCode.Success) // Run producer and consumer in parallel until done (likely by user cancelling with CTRL-C) .handleErrorWith { t =&gt; IO(println(s\"Error caught: ${t.getMessage}\")).as(ExitCode.Error) } } yield res } The full implementation of this naive producer consumer is available here. Our run function instantiates the shared queue wrapped in a Ref and boots the producer and consumer in parallel. To do to it uses parMapN, that creates and runs the fibers that will run the IOs passed as paremeter. Then it takes the output of each fiber and and applies a given function to them. In our case both producer and consumer shall run forever until user presses CTRL-C which will trigger a cancellation. Alternatively we could have used start method to explicitely create new Fiber instances that will run the producer and consumer, then use join to wait for them to finish, something like: def run(args: List[String]): IO[ExitCode] = for { queueR &lt;- Ref.of[IO, Queue[Int]](Queue.empty[Int]) producerFiber &lt;- producer(queueR, 0).start consumerFiber &lt;- consumer(queueR, 0).start _ &lt;- producerFiber.join _ &lt;- consumerFiber.join } yield ExitCode.Error Problem is, if there is an error in any of the fibers the join call will not hint it, nor it will return. In contrast parMapN does promote the error it finds to the caller. In general, if possible, programmers should prefer to use higher level commands such as parMapN or parSequence to deal with fibers. Ok, we stick to our implementation based on .parMapN. Are we done? Does it Work? Well, it works… but it is far from ideal. If we run it we will find that the producer runs faster than the consumer so the queue is constantly growing. And, even if that was not the case, we must realize that the consumer will be continually running regardless if there are elements in the queue, which is far from ideal. We will try to improve it in the next section using semaphores. Also we will use several consumers and producers to balance production and consumption rate. Bringing in semaphores As discussed in the first section of this tutorial, semaphores are used to control access to critical sections of our code that handle shared resources, when those sections can be run concurrently. Each semaphore contains a number that represents permits. To access a critical section permits have to be obtained, later to be released when the critical section is left. In our producer/consumer code we already protect access to the queue (our shared resource) using a Ref. But we can use semaphores on top of that to signal whether there are elements in the queue or not. This can be done by keeping in the semaphore a counter of filled buckets, which will be increased when adding elements to the queue, and decreased when elements are removed. Consumer code will be changed so it blocks when there are no elements to be retrieved (filled has zero permits). So now our producer and consumer look like this: import cats.effect.concurrent.{Ref, Semaphore} import cats.effect._ import cats.syntax.all._ import scala.collection.immutable.Queue def producer[F[_]: Sync: ContextShift](id: Int, queueR: Ref[F, Queue[Int]], counterR: Ref[F, Int], filled: Semaphore[F]): F[Unit] = (for { i &lt;- counterR.getAndUpdate(_ + 1) _ &lt;- queueR.getAndUpdate(_.enqueue(i)) _ &lt;- filled.release // Signal new item in queue _ &lt;- if(i % 10000 == 0) Sync[F].delay(println(s\"Producer $id has reached $i items\")) else Sync[F].unit _ &lt;- ContextShift[F].shift } yield ()) &gt;&gt; producer(id, queueR, counterR, filled) def consumer[F[_]: Sync: ContextShift](id: Int, queueR: Ref[F, Queue[Int]], filled: Semaphore[F]): F[Unit] = (for { _ &lt;- filled.acquire // Wait for some item in queue i &lt;- queueR.modify(_.dequeue.swap) _ &lt;- if(i % 10000 == 0) Sync[F].delay(println(s\"Consumer $id has reached $i items\")) else Sync[F].unit _ &lt;- ContextShift[F].shift } yield ()) &gt;&gt; consumer(id, queueR, filled) Note how the producer uses filled.release to signal that a new item is available in the queue by increasing the number of permits in the semaphore. Likewise, consumer uses filled.acquire which block if filled is zero (no elements in queue). If it is not zero (or as soon as it becomes &gt; 0) the call is unblocked and the number of permits in the semaphore is decreased. Now our consumer is not concerned about trying to read from an empty queue. If it got a ‘permit’ from the semaphore it means the queue will have an element the consumer can read. Finally we modify our main program so it instantiates the counter and queue references, alongside with the semaphore. Also it will create several consumers and producers, 10 of each, and will start all of them in parallel: import cats.effect.concurrent.{Ref, Semaphore} import cats.effect._ import cats.instances.list._ import cats.syntax.all._ import scala.collection.immutable.Queue object ProducerConsumer extends IOApp { def producer[F[_]: Sync: ContextShift](id: Int, queueR: Ref[F, Queue[Int]], counterR: Ref[F, Int], filled: Semaphore[F]): F[Unit] = ??? // As defined before def consumer[F[_]: Sync: ContextShift](id: Int, queueR: Ref[F, Queue[Int]], filled: Semaphore[F]): F[Unit] = ??? // As defined before override def run(args: List[String]): IO[ExitCode] = for { queueR &lt;- Ref.of[IO, Queue[Int]](Queue.empty[Int]) counterR &lt;- Ref.of[IO, Int](1) filled &lt;- Semaphore[IO](0) producers = List.range(1, 11).map(producer(_, queueR, counterR, filled)) // 10 producers consumers = List.range(1, 11).map(consumer(_, queueR, filled)) // 10 consumers res &lt;- (producers ++ consumers) .parSequence.as(ExitCode.Success) // Run producers and consumers in parallel until done (likely by user cancelling with CTRL-C) .handleErrorWith { t =&gt; IO(println(s\"Error caught: ${t.getMessage}\")).as(ExitCode.Error) } } yield res } The full implementation of this producer consumer with unbounded queue is available here. Producers and consumers are created as two list of IO instances. All of them are started in their own fiber by the call to parSequence, which will wait for all of them to finish and then return the value passed as parameter. As in the previous example it shall run forever until the user presses CTRL-C. Having several consumers and producers improves the balance between consumers and producers… but still, on the long run, queue tends to grow in size. To fix this we will ensure the size of the queue is bounded, so whenever that max size is reached producers will block as they consumers do when the queue is empty. Producer consumer with bounded queue We will use another semaphore empty to represent the amount of empty ‘buckets’ in the queue. It will mirror the behavior of filled. When a new item is enqueued a call to empty.release will signal that there is a new empty bucket in the queue. Likewise, the producers will request an empty bucket by calling empty.acquire prior to add new elements, blocking when empty number of permits is zero. Producer and consumer code now becomes: import cats.effect.concurrent.{Ref, Semaphore} import cats.effect._ import cats.syntax.all._ import scala.collection.immutable.Queue def producer[F[_]: Sync: ContextShift](id: Int, queueR: Ref[F, Queue[Int]], counterR: Ref[F, Int], empty: Semaphore[F], filled: Semaphore[F]): F[Unit] = (for { i &lt;- counterR.getAndUpdate(_ + 1) _ &lt;- empty.acquire // Wait for some bucket free in queue _ &lt;- queueR.getAndUpdate(_.enqueue(i)) _ &lt;- filled.release // Signal new item in queue _ &lt;- if(i % 10000 == 0) Sync[F].delay(println(s\"Producer $id has reached $i items\")) else Sync[F].unit _ &lt;- ContextShift[F].shift } yield ()) &gt;&gt; producer(id, queueR, counterR, empty, filled) def consumer[F[_]: Sync: ContextShift](id: Int, queueR: Ref[F, Queue[Int]], empty: Semaphore[F], filled: Semaphore[F]): F[Unit] = (for { _ &lt;- filled.acquire // Wait for some item in queue i &lt;- queueR.modify(_.dequeue.swap) _ &lt;- empty.release // Signal new bucket free in queue _ &lt;- if(i % 10000 == 0) Sync[F].delay(println(s\"Consumer $id has reached $i items\")) else Sync[F].unit _ &lt;- ContextShift[F].shift } yield ()) &gt;&gt; consumer(id, queueR, empty, filled) The main program is almost the same, we only add the creation of the empty semaphore setting as value the max size of the queue. So for a max size of 100 we would have: import cats.effect.concurrent.{Ref, Semaphore} import cats.effect._ import cats.instances.list._ import cats.syntax.all._ import scala.collection.immutable.Queue object ProducerConsumerBounded extends IOApp { def producer[F[_]: Sync: ContextShift](id: Int, queueR: Ref[F, Queue[Int]], counterR: Ref[F, Int], empty: Semaphore[F], filled: Semaphore[F]): F[Unit] = ??? // As defined before def consumer[F[_]: Sync: ContextShift](id: Int, queueR: Ref[F, Queue[Int]], empty: Semaphore[F], filled: Semaphore[F]): F[Unit] = ??? // As defined before override def run(args: List[String]): IO[ExitCode] = for { queueR &lt;- Ref.of[IO, Queue[Int]](Queue.empty[Int]) counterR &lt;- Ref.of[IO, Int](1) empty &lt;- Semaphore[IO](100) filled &lt;- Semaphore[IO](0) producers = List.range(1, 11).map(producer(_, queueR, counterR, empty, filled)) // 10 producers consumers = List.range(1, 11).map(consumer(_, queueR, empty, filled)) // 10 consumers res &lt;- (producers ++ consumers) .parSequence.as(ExitCode.Success) // Run producers and consumers in parallel until done (likely by user cancelling with CTRL-C) .handleErrorWith { t =&gt; IO(println(s\"Error caught: ${t.getMessage}\")).as(ExitCode.Error) } } yield res } The full implementation of this producer consumer with bounded queue is available here. On cancellation of producer and consumers In our producers/consumers code we have not dealt yet with cancellation. Truth is the only cancellation that can happen in those small programs is caused by the user ‘killing’ the main program, and in that case we are not much concerned about the consequences (the program has terminated after all). But in more complex settings we have to be careful as consumers and producers fibers can be cancelled at any time. And which will then be the consequences? Well, basically we would not be able to trust our semaphores to represent the amount of items and free buckets in the queue. See for example the code of a consumer in our bounded queue setting. It must make sure that whenever a filled permit is acquired then a) a new element is added and b) empty.release is called. A cancellation right before those actions will leave the semaphores with values that do not represent the internal state of the queue. To prevent this we must make sure that a call to a producer and a consumer is fully run without being cancelled. This can be done with Sync[F].uncancelable, which ensures that the F instance passed as parameter cannot be cancelled. So, for example, our consumer in the bounded queue example will look like: import cats.effect.concurrent.{Ref, Semaphore} import cats.effect.{ContextShift, ExitCode, IO, IOApp, Sync} import cats.syntax.all._ import scala.collection.immutable.Queue def consumer[F[_]: Sync: ContextShift](id: Int, queueR: Ref[F, Queue[Int]], empty: Semaphore[F], filled: Semaphore[F]): F[Unit] = Sync[F].uncancelable( for { _ &lt;- filled.acquire // Wait for some item in queue i &lt;- queueR.modify(_.dequeue.swap) _ &lt;- empty.release _ &lt;- if(i % 10000 == 0) Sync[F].delay(println(s\"Consumer $id has reached $i items\")) else Sync[F].unit _ &lt;- ContextShift[F].shift } yield () ) &gt;&gt; consumer(id, queueR, empty, filled) Making the producer function uncancelable is similar, we leave that as a small exercise. Exercise: build a concurrent queue A concurrent queue is, well, a queue data structure that allows safe concurrent access. That is, several concurrent processes can safely add and retrieve data from the queue. It is easy to realize that during the previous sections we already implemented that kind of functionality, it was embedded in our producer and consumer functions. To build a concurrent queue we only need to extract from those methods the part that handles the concurrent access. A simple concurrent queue with only two methods could be defined as: import cats.effect.{Concurrent, Sync} trait SimpleConcurrentQueue[F[_], A] { /** Get and remove first element from queue, blocks if queue empty. */ def poll: F[A] /** Put element at then end of queue, blocks if queue is bounded and full.*/ def put(a: A): F[Unit] } // Exercise: implement builder methods in this companion object object SimpleConcurrentQueue { def unbounded[F[_]: Concurrent: Sync, A]: F[SimpleConcurrentQueue[F, A]] = ??? def bounded[F[_]: Concurrent: Sync, A](size: Int): F[SimpleConcurrentQueue[F, A]] = ??? } The exercise is to implement the missing constructor methods we have included in SimpleConcurrentQueue companion object. A possible implementation is given here for reference. Finally, we propose you to write a more complete concurrent queue implementation with these definitions: import cats.effect.{Concurrent, Sync} trait ConcurrentQueue[F[_], A] { /** Get and remove first element from queue, blocks if queue empty. */ def poll: F[A] /** Get and remove first `n` elements from queue, blocks if less than `n` items are available in queue. * Error raised if `n &lt; 0` or, in bounded queues, if `n &gt; max size of queue`.*/ def pollN(n: Int): F[List[A]] /** Get, but not remove, first element in queue, blocks if queue empty. */ def peek: F[A] /** Get, but not remove, first `n` elements in queue, blocks if less than `n` items are available in queue. * Error raised if `n &lt; 0` or, in bounded queues, if `n &gt; max size of queue`.*/ def peekN(n: Int): F[List[A]] /** Put element at then end of queue, blocks if queue is bounded and full. */ def put(a: A): F[Unit] /** Puts elements at the end of the queue, blocks if queue is bounded and does not have spare size for all items. * Error raised in bounded queues if `as.size &gt; max size of queue`.*/ def putN(as: List[A]): F[Unit] /** Try to get and remove first element from queue, immediately returning `F[None]` if queue empty. Non-blocking. */ def tryPoll: F[Option[A]] /** Try to get and remove first `n` elements from queue, immediately returning `F[None]` if less than `n` items are available in queue. Non-blocking. * Error raised if n &lt; 0. */ def tryPollN(n: Int): F[Option[List[A]]] /** Try to get, but not remove, first element from queue, immediately returning `F[None]` if queue empty. Non-blocking. */ def tryPeek: F[Option[A]] /** Try to get, but not remove, first `n` elements from queue, immediately returning `F[None]` if less than `n` items are available in queue. Non-blocking. * Error raised if n &lt; 0. */ def tryPeekN(n: Int): F[Option[List[A]]] /** Try to put element at the end of queue, immediately returning `F[false]` if queue is bounded and full. Non-blocking. */ def tryPut(a: A): F[Boolean] /** Try to put elements in list at the end of queue, immediately returning `F[false]` if queue is bounded and does not have spare size for all items. Non-blocking. */ def tryPutN(as: List[A]): F[Boolean] /** Returns # of items in queue. Non-blocking. */ def size: F[Long] /** Returns `F[true]` if queue empty, `F[false]` otherwise. Non-blocking. */ def isEmpty: F[Boolean] } /** * Specific methods for bounded concurrent queues. */ trait BoundedConcurrentQueue[F[_], A] extends ConcurrentQueue [F, A] { /** Max queue size. */ val maxQueueSize: Int /** Remaining empty buckets. Non-blocking.*/ def emptyBuckets: F[Long] /** Returns `F[true]` if queue full, `F[false]` otherwise. Non-blocking. */ def isFull: F[Boolean] } // Exercise: implement builder methods in this companion object object ConcurrentQueue { def unbounded[F[_]: Concurrent: Sync, A]: F[ConcurrentQueue[F, A]] = ??? def bounded[F[_]: Concurrent: Sync, A](size: Int): F[BoundedConcurrentQueue[F, A]] = ??? } Note that now bounded concurrent queues have their own type BoundedConcurrentQueue which extends ConcurrentQueue with some functions specific to them. As before, you should try to implement the builder methods in companion object ConcurrentQueue. TIP: we have not reviewed all methods that cats-effect’s Semaphore has to offer. Take a look to Semaphore API as you will find some new useful methods there. A possible implementation of the concurrent queue is given here for reference. NOTE this concurrent queue implementation is not intended for high-throughput requirements, and it has not been tested in production environments. For a production-ready, high-performance concurrent queue it is strongly advised to consider Monix’s concurrent queue, which is cats-effect compatible. Conclusion With all this we have covered a good deal of what cats-effect has to offer (but not all!). Now you are ready to use to create code that operate side effects in a purely functional manner. Enjoy the ride!"
    }    
  ];

  idx = lunr(function () {
    this.ref("title");
    this.field("content");

    docs.forEach(function (doc) {
      this.add(doc);
    }, this);
  });

  docs.forEach(function (doc) {
    docMap.set(doc.title, doc.url);
  });
}

// The onkeypress handler for search functionality
function searchOnKeyDown(e) {
  const keyCode = e.keyCode;
  const parent = e.target.parentElement;
  const isSearchBar = e.target.id === "search-bar";
  const isSearchResult = parent ? parent.id.startsWith("result-") : false;
  const isSearchBarOrResult = isSearchBar || isSearchResult;

  if (keyCode === 40 && isSearchBarOrResult) {
    // On 'down', try to navigate down the search results
    e.preventDefault();
    e.stopPropagation();
    selectDown(e);
  } else if (keyCode === 38 && isSearchBarOrResult) {
    // On 'up', try to navigate up the search results
    e.preventDefault();
    e.stopPropagation();
    selectUp(e);
  } else if (keyCode === 27 && isSearchBarOrResult) {
    // On 'ESC', close the search dropdown
    e.preventDefault();
    e.stopPropagation();
    closeDropdownSearch(e);
  }
}

// Search is only done on key-up so that the search terms are properly propagated
function searchOnKeyUp(e) {
  // Filter out up, down, esc keys
  const keyCode = e.keyCode;
  const cannotBe = [40, 38, 27];
  const isSearchBar = e.target.id === "search-bar";
  const keyIsNotWrong = !cannotBe.includes(keyCode);
  if (isSearchBar && keyIsNotWrong) {
    // Try to run a search
    runSearch(e);
  }
}

// Move the cursor up the search list
function selectUp(e) {
  if (e.target.parentElement.id.startsWith("result-")) {
    const index = parseInt(e.target.parentElement.id.substring(7));
    if (!isNaN(index) && (index > 0)) {
      const nextIndexStr = "result-" + (index - 1);
      const querySel = "li[id$='" + nextIndexStr + "'";
      const nextResult = document.querySelector(querySel);
      if (nextResult) {
        nextResult.firstChild.focus();
      }
    }
  }
}

// Move the cursor down the search list
function selectDown(e) {
  if (e.target.id === "search-bar") {
    const firstResult = document.querySelector("li[id$='result-0']");
    if (firstResult) {
      firstResult.firstChild.focus();
    }
  } else if (e.target.parentElement.id.startsWith("result-")) {
    const index = parseInt(e.target.parentElement.id.substring(7));
    if (!isNaN(index)) {
      const nextIndexStr = "result-" + (index + 1);
      const querySel = "li[id$='" + nextIndexStr + "'";
      const nextResult = document.querySelector(querySel);
      if (nextResult) {
        nextResult.firstChild.focus();
      }
    }
  }
}

// Search for whatever the user has typed so far
function runSearch(e) {
  if (e.target.value === "") {
    // On empty string, remove all search results
    // Otherwise this may show all results as everything is a "match"
    applySearchResults([]);
  } else {
    const tokens = e.target.value.split(" ");
    const moddedTokens = tokens.map(function (token) {
      // "*" + token + "*"
      return token;
    })
    const searchTerm = moddedTokens.join(" ");
    const searchResults = idx.search(searchTerm);
    const mapResults = searchResults.map(function (result) {
      const resultUrl = docMap.get(result.ref);
      return { name: result.ref, url: resultUrl };
    })

    applySearchResults(mapResults);
  }

}

// After a search, modify the search dropdown to contain the search results
function applySearchResults(results) {
  const dropdown = document.querySelector("div[id$='search-dropdown'] > .dropdown-content.show");
  if (dropdown) {
    //Remove each child
    while (dropdown.firstChild) {
      dropdown.removeChild(dropdown.firstChild);
    }

    //Add each result as an element in the list
    results.forEach(function (result, i) {
      const elem = document.createElement("li");
      elem.setAttribute("class", "dropdown-item");
      elem.setAttribute("id", "result-" + i);

      const elemLink = document.createElement("a");
      elemLink.setAttribute("title", result.name);
      elemLink.setAttribute("href", result.url);
      elemLink.setAttribute("class", "dropdown-item-link");

      const elemLinkText = document.createElement("span");
      elemLinkText.setAttribute("class", "dropdown-item-link-text");
      elemLinkText.innerHTML = result.name;

      elemLink.appendChild(elemLinkText);
      elem.appendChild(elemLink);
      dropdown.appendChild(elem);
    });
  }
}

// Close the dropdown if the user clicks (only) outside of it
function closeDropdownSearch(e) {
  // Check if where we're clicking is the search dropdown
  if (e.target.id !== "search-bar") {
    const dropdown = document.querySelector("div[id$='search-dropdown'] > .dropdown-content.show");
    if (dropdown) {
      dropdown.classList.remove("show");
      document.documentElement.removeEventListener("click", closeDropdownSearch);
    }
  }
}
